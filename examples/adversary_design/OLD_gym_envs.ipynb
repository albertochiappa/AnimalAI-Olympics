{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controlled-collapse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from animalai.envs.arena_config import Vector3, RGB, Item, Arena, ArenaConfig\n",
    "from animalai.envs.environment import AnimalAIEnvironment\n",
    "from mlagents_envs.exception import UnityCommunicationException\n",
    "\n",
    "from typing import List\n",
    "from animalai.communicator_objects import (\n",
    "    ArenasConfigurationsProto,\n",
    "    ArenaConfigurationProto,\n",
    "    ItemToSpawnProto,\n",
    "    VectorProto,\n",
    ")\n",
    "\n",
    "from typing import List\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import networkx as nx\n",
    "from networkx import grid_graph\n",
    "\n",
    "import gym\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-batman",
   "metadata": {},
   "source": [
    "# PAIRED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-parks",
   "metadata": {},
   "source": [
    "In this section we create the gym environment as in PAIRED: https://github.com/google-research/google-research/blob/master/social_rl/gym_multigrid/envs/adversarial.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "utility-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialEnv(gym.Env):\n",
    "    ''' Grid world where an adversary build the environment the agent plays.\n",
    "    The adversary places the goal, agent, and up to n_clutter blocks in sequence.\n",
    "    The action dimension is the number of squares in the grid, and each action\n",
    "    chooses where the next item should be placed. '''\n",
    "\n",
    "    def __init__(self, n_clutter=10, size=40, goal_noise=0., random_z_dim=10, choose_goal_last=False):\n",
    "        '''Initializes environment in which adversary places goal, agent, obstacles.\n",
    "        Args:\n",
    "          n_clutter: The maximum number of obstacles the adversary can place.\n",
    "          size: The number of tiles across one side of the grid; i.e. make a size x size grid.\n",
    "          max_steps: The maximum number of steps that can be taken before the episode terminates.\n",
    "          goal_noise: The probability with which the goal will move to a different location than \n",
    "              the one chosen by the adversary.\n",
    "          random_z_dim: The environment generates a random vector z to condition the adversary. \n",
    "              This gives the dimension of that vector.\n",
    "          choose_goal_last: If True, will place the goal and agent as the last actions, rather than the first actions.\n",
    "        '''\n",
    "        # define params\n",
    "        self.agent_start_pos = None\n",
    "        self.goal_pos = None\n",
    "        self.n_clutter = n_clutter\n",
    "        self.goal_noise = goal_noise # we do not actualy use it (not yet)\n",
    "        self.random_z_dim = random_z_dim\n",
    "        self.choose_goal_last = choose_goal_last\n",
    "        # Add two actions for placing the agent and goal.\n",
    "        self.adversary_max_steps = self.n_clutter + 2\n",
    "        self.width=size\n",
    "        self.height=size\n",
    "        \n",
    "        self.reset_metrics()\n",
    "\n",
    "        # Create spaces for adversary agent's specs: all spaces where it can place an object\n",
    "        self.adversary_action_dim = size**2\n",
    "        self.adversary_action_space = gym.spaces.Discrete(self.adversary_action_dim)\n",
    "        self.adversary_ts_obs_space = gym.spaces.Box(\n",
    "            low=0, high=self.adversary_max_steps, shape=(1,), dtype='uint8')\n",
    "        self.adversary_randomz_obs_space = gym.spaces.Box(\n",
    "            low=0, high=1.0, shape=(random_z_dim,), dtype=np.float32)\n",
    "        # obs space: 0 if arena, 1 agent, 2 goal, walls 3\n",
    "        self.adversary_image_obs_space = gym.spaces.Box(low=0,high=3,shape=(self.width, self.height, 1),dtype='uint8')\n",
    "\n",
    "        # Adversary observations are dictionaries containing an encoding of the grid, the current time step, \n",
    "        # and a randomly generated vector used to condition generation (as in a GAN).\n",
    "        self.adversary_observation_space = gym.spaces.Dict(\n",
    "            {'image': self.adversary_image_obs_space,\n",
    "             'time_step': self.adversary_ts_obs_space,\n",
    "             'random_z': self.adversary_randomz_obs_space})\n",
    "\n",
    "    def _gen_grid(self):\n",
    "        \"\"\"Grid is initially empty, because adversary will create it.\"\"\"\n",
    "        # Create an empty grid\n",
    "        self.grid = np.zeros((self.width, self.height))\n",
    "        \n",
    "    def get_goal(self):\n",
    "        if self.goal_pos is None:\n",
    "            return -1\n",
    "        return self.goal_pos[0]\n",
    "\n",
    "    def get_agent(self):\n",
    "        if self.goal_pos is None:\n",
    "            return -1\n",
    "        return self.agent_start_pos\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        '''Define other metrics'''\n",
    "        self.distance_to_goal = -1\n",
    "        self.n_clutter_placed = 0\n",
    "        self.deliberate_agent_placement = -1\n",
    "        self.passable = -1\n",
    "        self.adversary_step_count = 0\n",
    "        self.graph = grid_graph(dim=[self.width, self.height])\n",
    "        self.wall_locs = []\n",
    "        \n",
    "    def reset(self):\n",
    "        '''Fully resets the environment to an empty grid with no agent or goal.'''\n",
    "        \n",
    "        self.adversary_step_count = 0\n",
    "\n",
    "        self.agent_start_pos = None\n",
    "        self.goal_pos = None\n",
    "\n",
    "        # Extra metrics\n",
    "        self.reset_metrics()\n",
    "\n",
    "        # Generate the empty grid\n",
    "        self._gen_grid()\n",
    "\n",
    "        #image = self.grid.encode()\n",
    "        obs = {\n",
    "            'image': self.grid,\n",
    "            'time_step': [self.adversary_step_count],\n",
    "            'random_z': self.generate_random_z()\n",
    "        }\n",
    "        \n",
    "        done = False\n",
    "\n",
    "        return obs\n",
    "    \n",
    "    def generate_random_z(self):\n",
    "        return np.random.uniform(size=(self.random_z_dim,)).astype(np.float32)\n",
    "    \n",
    "    def remove_wall(self, x, y):\n",
    "        # if there is a wall, remove it\n",
    "        if self.grid[x, y] == 3:\n",
    "            self.grid[x, y] = 0\n",
    "            \n",
    "    def render(self):\n",
    "        fig = plt.imshow(self.grid, cmap=plt.get_cmap('Accent'))\n",
    "        # values\n",
    "        values = np.array([0, 1, 2, 3])\n",
    "        #items\n",
    "        items = ['Arena', 'Agent', 'Goal', 'Wall']\n",
    "        # colormap used by imshow\n",
    "        colors = [fig.cmap(fig.norm(value)) for value in values]\n",
    "        # create a patch (proxy artist) for every color \n",
    "        patches = [mpatches.Patch(color=colors[i], label=\"{l}\".format(l=items[i]) ) for i in range(len(values)) ]\n",
    "        # put those patched as legend-handles into the legend\n",
    "        plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\n",
    "\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "        \n",
    "    def compute_shortest_path(self):\n",
    "        if self.agent_start_pos is None or self.goal_pos is None:\n",
    "            return\n",
    "\n",
    "        self.distance_to_goal = abs(\n",
    "            self.goal_pos[0] - self.agent_start_pos[0]) + abs(\n",
    "                self.goal_pos[1] - self.agent_start_pos[1])\n",
    "\n",
    "        # Check if there is a path between agent start position and goal. Remember\n",
    "        # to subtract 1 due to outside walls existing in the Grid, but not in the\n",
    "        # networkx graph.\n",
    "        self.passable = nx.has_path(\n",
    "            self.graph,\n",
    "            source=(self.agent_start_pos[0], self.agent_start_pos[1]),\n",
    "            target=(self.goal_pos[0], self.goal_pos[1]))\n",
    "        if self.passable:\n",
    "          # Compute shortest path\n",
    "            self.shortest_path_length = nx.shortest_path_length(\n",
    "              self.graph,\n",
    "              source=(self.agent_start_pos[0], self.agent_start_pos[1]),\n",
    "              target=(self.goal_pos[0], self.goal_pos[1]))\n",
    "        else:\n",
    "          # Impassable environments have a shortest path length 1 longer than\n",
    "          # longest possible path\n",
    "            self.shortest_path_length = (self.width - 1) * (self.height - 1) + 1\n",
    "    \n",
    "    def step(self, loc):\n",
    "    #def step(self, x, y):\n",
    "        '''The adversary gets n_clutter + 2 moves to place the goal, agent, blocks.\n",
    "        The action space is the number of possible squares in the grid. \n",
    "        The squares are numbered from left to right, top to bottom.\n",
    "        Args:\n",
    "          loc: An integer specifying the location to place the next object which\n",
    "            must be decoded into x, y coordinates.\n",
    "        Returns:\n",
    "          Standard RL observation, reward (always 0), done, and info\n",
    "        '''\n",
    "        #if loc >= self.adversary_action_dim:\n",
    "         #   raise ValueError('Position passed to step_adversary is outside the grid.')\n",
    "\n",
    "        x = int(loc % self.width) \n",
    "        y = int(loc / self.width) \n",
    "        #done = False\n",
    "        \n",
    "        # Check if we should choose goal or agent\n",
    "        if self.choose_goal_last:\n",
    "            should_choose_goal = self.adversary_step_count == self.adversary_max_steps - 2\n",
    "            should_choose_agent = self.adversary_step_count == self.adversary_max_steps - 1\n",
    "        else:\n",
    "            should_choose_goal = self.adversary_step_count == 0\n",
    "            should_choose_agent = self.adversary_step_count == 1\n",
    "        \n",
    "        if should_choose_goal:\n",
    "            self.grid[x, y] = 1\n",
    "            self.goal_pos = [x, y]\n",
    "        elif should_choose_agent:\n",
    "            if self.grid[x, y] != 1:\n",
    "                self.grid[x, y] = 2\n",
    "                self.agent_start_pos = [x, y]\n",
    "            else:\n",
    "                while self.grid[x, y] == 1:\n",
    "                    x = randint(1, 14)\n",
    "                    y = randint(1, 14)\n",
    "                    self.grid[x, y] = 2\n",
    "                    self.agent_start_pos = [x, y]           \n",
    "        else:\n",
    "            if self.grid[x, y] != 1 and self.grid[x, y] != 2:\n",
    "                self.grid[x, y] = 3\n",
    "                if (x, y) not in self.wall_locs:\n",
    "                    self.wall_locs.append((x, y))\n",
    "            \n",
    "        self.adversary_step_count += 1\n",
    "            \n",
    "        # End of episode\n",
    "        if self.adversary_step_count >= self.adversary_max_steps:\n",
    "            done = True\n",
    "            \n",
    "            # Build graph after we are certain agent and goal are placed\n",
    "            for w in self.wall_locs:\n",
    "                self.graph.remove_node(w)\n",
    "            self.compute_shortest_path()\n",
    "\n",
    "        obs = {\n",
    "            'image': self.grid,\n",
    "            'time_step': [self.adversary_step_count],\n",
    "            'random_z': self.generate_random_z()\n",
    "        }\n",
    "\n",
    "        #return obs, env.shortest_path_length  if done else 0, done, {}\n",
    "        return obs, 0, done, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-elder",
   "metadata": {},
   "source": [
    "## more simple + inlcuding protagonist trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "domestic-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "occasional-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "import networkx as nx\n",
    "from networkx import grid_graph\n",
    "\n",
    "class Adversarial_v2(gym.Env):\n",
    "    ''' Grid world where an adversary build the environment the agent plays.\n",
    "    The adversary places the goal, agent, and up to n_clutter blocks in sequence.\n",
    "    The action dimension is the number of squares in the grid, and each action\n",
    "    chooses where the next item should be placed. '''\n",
    "\n",
    "    def __init__(self, n_clutter=1, size=38):\n",
    "        '''Initializes environment in which adversary places goal, agent, obstacles.\n",
    "        Args:\n",
    "          n_clutter: The maximum number of obstacles the adversary can place.\n",
    "          size: The number of tiles across one side of the grid; i.e. make a size x size grid.\n",
    "          max_steps: The maximum number of steps that can be taken before the episode terminates.\n",
    "        '''\n",
    "        # define params\n",
    "        self.n_clutter = n_clutter\n",
    "        # Add two actions for placing the agent and goal.\n",
    "        self.MAX_STEPS = self.n_clutter + 2\n",
    "        self.size=size\n",
    "        \n",
    "        # to train protagonist\n",
    "        self.n_episodes = 0\n",
    "        self.load_model = False\n",
    "        self.base_port_protagonist = 4000\n",
    "        self.n_steps_trainer = 10000\n",
    "\n",
    "        # Create spaces for adversary agent's specs: all spaces where it can place an object\n",
    "        self.action_space = gym.spaces.Discrete(size**2)\n",
    "        \n",
    "        # observation step: image + time step\n",
    "        self.image_space = gym.spaces.Box(low=0,high=3,shape=(self.size, self.size),dtype=np.float16)\n",
    "        self.ts_space = gym.spaces.Box(low=0, high=self.MAX_STEPS, shape=(1,), dtype='uint8')\n",
    "        \n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {'image': self.image_space,\n",
    "             'time_step': self.ts_space})\n",
    "        \n",
    "        self.seed()\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.agent_start_pos = -1\n",
    "        self.goal_pos = -1\n",
    "        self.count = 0    \n",
    "        self.image_space = np.zeros((self.size, self.size))\n",
    "        self.state = {\n",
    "            'image': self.image_space,\n",
    "            'time_step': [self.count]\n",
    "        }\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "        self.info = {}\n",
    "        self.wall_locs = []\n",
    "        self.graph = grid_graph(dim=[self.size, self.size])\n",
    "        self.passable = -1\n",
    "        self.distance_to_goal = None\n",
    "        self.shortest_path_length = None\n",
    "                \n",
    "        return self.state\n",
    "        \n",
    "    def compute_shortest_path(self):\n",
    "        if self.agent_start_pos is None or self.goal_pos is None:\n",
    "            return\n",
    "\n",
    "        self.distance_to_goal = abs(\n",
    "            self.goal_pos[0] - self.agent_start_pos[0]) + abs(\n",
    "                self.goal_pos[1] - self.agent_start_pos[1])\n",
    "\n",
    "        # Check if there is a path between agent start position and goal\n",
    "        self.passable = nx.has_path(\n",
    "            self.graph,\n",
    "            source=(self.agent_start_pos[0], self.agent_start_pos[1]),\n",
    "            target=(self.goal_pos[0], self.goal_pos[1]))\n",
    "        if self.passable:\n",
    "          # Compute shortest path\n",
    "            self.shortest_path_length = nx.shortest_path_length(\n",
    "              self.graph,\n",
    "              source=(self.agent_start_pos[0], self.agent_start_pos[1]),\n",
    "              target=(self.goal_pos[0], self.goal_pos[1]))\n",
    "        else:\n",
    "          # Impassable environments have a shortest path length 1 longer than the longest possible path\n",
    "            self.shortest_path_length = (self.size-1) * (self.size-1) + 1\n",
    "        \n",
    "    def step (self, action):\n",
    "        if self.done:\n",
    "          # should never reach this point\n",
    "            print(\"EPISODE DONE!!!\")\n",
    "        elif self.count == self.MAX_STEPS:\n",
    "            \n",
    "            for w in self.wall_locs:\n",
    "                self.graph.remove_node(w)\n",
    "            self.compute_shortest_path()\n",
    "            print('Shortest path: ', self.shortest_path_length )\n",
    "            \n",
    "            # padd with zeros observation space (so before we did not consider walls)\n",
    "            matrix = np.zeros((40, 40))\n",
    "            matrix[1:39,1:39] = env.image_space \n",
    "            \n",
    "            # create AnimalAI environment configuration from matrix\n",
    "            arena_config = matrix2arena(matrix)\n",
    "            \n",
    "            # run protagonist training\n",
    "            reward = train_protagonist(arena_config=arena_config, n_steps_trainer=self.n_steps_trainer, \\\n",
    "                                       base_port_protagonist=self.base_port_protagonist + self.n_episodes, \\\n",
    "                                       load_model=self.load_model)\n",
    "            \n",
    "            # update trainer (add steps)\n",
    "            self.n_episodes += 1\n",
    "            self.load_model = True # now we can load the model\n",
    "            \n",
    "            \n",
    "            self.reward = reward\n",
    "            self.done = True\n",
    "            \n",
    "              \n",
    "        else:\n",
    "            try:\n",
    "                assert self.action_space.contains(action)\n",
    "                \n",
    "                x = int(action % self.size) \n",
    "                y = int(action / self.size) \n",
    "\n",
    "                should_choose_goal = self.count == 0\n",
    "                should_choose_agent = self.count == 1\n",
    "                \n",
    "                if should_choose_goal:\n",
    "                    self.image_space[x, y] = 1\n",
    "                    self.goal_pos = [x, y]\n",
    "                elif should_choose_agent:\n",
    "                    if self.image_space[x, y] != 1:\n",
    "                        self.image_space[x, y] = 2\n",
    "                    else:\n",
    "                        while self.image_space[x, y] == 1:\n",
    "                            l = randint(0, self.size**2)\n",
    "                            x = int(l % self.size) \n",
    "                            y = int(l / self.size)\n",
    "                        self.image_space[x, y] = 2\n",
    "                    self.agent_start_pos = [x, y]    \n",
    "                else:\n",
    "                    if self.image_space[x, y] != 1 and self.image_space[x, y] != 2:\n",
    "                        self.image_space[x, y] = 3\n",
    "                        if (x, y) not in self.wall_locs:\n",
    "                            self.wall_locs.append((x, y))\n",
    "                self.count += 1\n",
    "                \n",
    "            except AssertionError:\n",
    "                print(\"INVALID ACTION\", action)  \n",
    "                \n",
    "         \n",
    "        self.state = {\n",
    "            'image': self.image_space,\n",
    "            'time_step': [self.count]\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            assert self.observation_space.contains(self.state)\n",
    "        except AssertionError:\n",
    "            print(\"INVALID STATE\", self.state)  \n",
    "        \n",
    "        return self.state, self.reward, self.done, self.info\n",
    "    \n",
    "    def render(self, mode=\"human\"):\n",
    "        s = \"reward: {:2d}  info: {}\"\n",
    "        print(s.format(self.reward, self.info))\n",
    "        \n",
    "    def seed (self, seed=None):\n",
    "        \"\"\"Sets the seed for this env's random number generator(s).\"\"\"\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "surgical-island",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest path:  26\n",
      "Converting ./models/protagonist/AnimalAI/frozen_graph_def.pb to ./models/protagonist/AnimalAI.nn\n",
      "GLOBALS: 'is_continuous_control', 'version_number', 'memory_size', 'action_output_shape'\n",
      "IN: 'visual_observation_0': [-1, 84, 84, 3] => 'policy/main_graph_0_encoder0/conv_1/BiasAdd'\n",
      "IN: 'vector_observation': [-1, 1, 1, 3] => 'policy/main_graph_0/hidden_0/BiasAdd'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice_1'\n",
      "OUT: 'policy/concat/concat', 'action'\n",
      "DONE: wrote ./models/protagonist/AnimalAI.nn file.\n",
      "PROTAGONIST: \n",
      "Steps:  10000  Cumulative reward:  -0.9472832645464224  Episode Length:  247.475\n",
      "Steps:  20000  Cumulative reward:  -0.6517518694014397  Episode Length:  231.2093023255814\n",
      "Steps:  30000  Cumulative reward:  -0.4012295642246802  Episode Length:  220.9333333333333\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Adversarial_v2' object has no attribute 'trainer_config_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c64a526f83d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#loc = randint(0, size**2-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-6ab49de3930d>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;31m# update trainer (add steps)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mupdate_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_config_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_episodes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m# now we can load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Adversarial_v2' object has no attribute 'trainer_config_path'"
     ]
    }
   ],
   "source": [
    "size = 38\n",
    "n_blocks = 1\n",
    "env = Adversarial_v2(n_clutter=n_blocks, size=size)\n",
    "for i in range(10):\n",
    "    #loc = randint(0, size**2-1)\n",
    "    loc = env.action_space.sample()\n",
    "    env.step(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "occupied-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "disabled-lesson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnimalAI:\n",
      "  batch_size: 64\n",
      "  beta: 0.01\n",
      "  buffer_size: 2024\n",
      "  epsilon: 0.2\n",
      "  hidden_units: 256\n",
      "  lambd: 0.95\n",
      "  learning_rate: 0.0003\n",
      "  learning_rate_schedule: linear\n",
      "  max_steps: 10000\n",
      "  memory_size: 128\n",
      "  normalize: false\n",
      "  num_epoch: 3\n",
      "  num_layers: 1\n",
      "  reward_signals:\n",
      "    curiosity:\n",
      "      encoding_size: 256\n",
      "      gamma: 0.99\n",
      "      strength: 0.01\n",
      "    extrinsic:\n",
      "      gamma: 0.99\n",
      "      strength: 1.0\n",
      "  sequence_length: 64\n",
      "  summary_freq: 10000\n",
      "  time_horizon: 128\n",
      "  trainer: ppo\n",
      "  use_recurrent: false\n",
      "  vis_encode_type: simple\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../configurations/training_configurations/train_ml_agents_config_ppo_10fs.yaml\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rotary-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True\n",
    "trainer_config_path = \"../configurations/training_configurations/train_ml_agents_config_ppo_10fs.yaml\"\n",
    "n_steps_trainer = 10000\n",
    "if load_model:\n",
    "    reset_trainer(trainer_config_path, steps=n_steps_trainer)\n",
    "else:\n",
    "    update_trainer(trainer_config_path, steps_add=n_steps_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "reverse-example",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize(env.image_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "spatial-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = matrix2arena(env.image_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "revised-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_path = (\n",
    "    \"../configurations/training_configurations/train_ml_agents_config_ppo_10fs.yaml\"\n",
    ")\n",
    "reset_trainer(trainer_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "vocational-option",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting ./models/protagonist/AnimalAI/frozen_graph_def.pb to ./models/protagonist/AnimalAI.nn\n",
      "GLOBALS: 'is_continuous_control', 'version_number', 'memory_size', 'action_output_shape'\n",
      "IN: 'visual_observation_0': [-1, 84, 84, 3] => 'policy/main_graph_0_encoder0/conv_1/BiasAdd'\n",
      "IN: 'vector_observation': [-1, 1, 1, 3] => 'policy/main_graph_0/hidden_0/BiasAdd'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice_1'\n",
      "OUT: 'policy/concat/concat', 'action'\n",
      "DONE: wrote ./models/protagonist/AnimalAI.nn file.\n",
      "PROTAGONIST: \n",
      "Steps:  10000  Cumulative reward:  -0.9999999310821296  Episode Length:  248.0\n",
      "Steps:  10000  Cumulative reward:  -0.9999999310821296  Episode Length:  248.0\n",
      "Steps:  10000  Cumulative reward:  -0.9999999310821296  Episode Length:  248.0\n"
     ]
    }
   ],
   "source": [
    "reward = train_protagonist(arena_config=my_config, trainer_config_path=trainer_config_path, base_port_protagonist=6002, load_model=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
