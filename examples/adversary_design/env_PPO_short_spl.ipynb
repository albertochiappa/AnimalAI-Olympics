{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accepted-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_example\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-programmer",
   "metadata": {},
   "source": [
    "# Adversary example: shortest path between agent and goal\n",
    "\n",
    "This notebook trains the adversary to create the shortest path possible between agent and goal. Only one wall is used. Grid size 38x38 in adversarial-v2 reward is set to: self.reward = - (self.shortest_path_length - 25)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-aggregate",
   "metadata": {},
   "source": [
    "## Checking that everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "improved-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"adversarial-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coupled-xerox",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(state):\n",
    "    fig = plt.imshow(state, cmap=plt.get_cmap('Accent'))\n",
    "    # values\n",
    "    values = np.array([0, 1, 2, 3])\n",
    "    #items\n",
    "    items = ['Arena', 'Agent', 'Goal', 'Wall']\n",
    "    # colormap used by imshow\n",
    "    colors = [fig.cmap(fig.norm(value)) for value in values]\n",
    "    # create a patch (proxy artist) for every color \n",
    "    patches = [mpatches.Patch(color=colors[i], label=\"{l}\".format(l=items[i]) ) for i in range(len(values)) ]\n",
    "    # put those patched as legend-handles into the legend\n",
    "    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\n",
    "\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "potential-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_episode(env, verbose=False):\n",
    "    env.reset()\n",
    "    sum_reward = 0\n",
    "\n",
    "    for i in range(env.MAX_STEPS+1):\n",
    "        action = env.action_space.sample()\n",
    "        if verbose:\n",
    "            print(\"action:\", action)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        sum_reward += reward\n",
    "\n",
    "        if verbose:\n",
    "            env.render()\n",
    "\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(\"done @ step {}\".format(i))\n",
    "            break\n",
    "\n",
    "    if verbose:\n",
    "        print(\"cumulative reward\", sum_reward)\n",
    "\n",
    "    return sum_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "curious-lincoln",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum reward:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"adversarial-v2\")\n",
    "sum_reward = run_one_episode(env)\n",
    "print('Sum reward: ', sum_reward)\n",
    "\n",
    "#visualize(env.state) --> if adversarial_v0\n",
    "\n",
    "visualize(env.image_space) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bacterial-desktop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'image': array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.]]),\n",
       "  'time_step': [2]},\n",
       " 0,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"adversarial-v2\")\n",
    "env.step(0)\n",
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "excellent-purchase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "baseline cumulative reward:  1e+01\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "\n",
    "for _ in range(20):\n",
    "    sum_reward = run_one_episode(env, verbose=False)\n",
    "    history.append(sum_reward)\n",
    "\n",
    "avg_sum_reward = sum(history) / len(history)\n",
    "print(\"\\nbaseline cumulative reward: {:6.2}\".format(avg_sum_reward))\n",
    "\n",
    "#history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "logical-combine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.image_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-restaurant",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Initialize the directory in which to save checkpoints (i.e., serialize a policy to disk) as a subdirectory ./tmp/exa and also the directory in which to write the logs which Ray expects to be at ~/ray_results/ by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "satisfied-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "chkpt_root = \"tmp/exa\"\n",
    "\n",
    "shutil.rmtree(chkpt_root, ignore_errors=True, onerror=None)\n",
    "ray_results = \"{}/ray_results/\".format(os.getenv(\"HOME\"))\n",
    "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-envelope",
   "metadata": {},
   "source": [
    "We’ll start Ray running in local mode, i.e., not running on a remote cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "attached-blanket",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-21 16:10:30,699\tINFO services.py:1174 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '128.179.144.248',\n",
       " 'raylet_ip_address': '128.179.144.248',\n",
       " 'redis_address': '128.179.144.248:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-04-21_16-10-30_180051_6081/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-04-21_16-10-30_180051_6081/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-04-21_16-10-30_180051_6081',\n",
       " 'metrics_export_port': 49593,\n",
       " 'node_id': 'b4a6ea629362e21d87711d9f448dc2834afc1bc41ed7078a28443683'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(ignore_reinit_error=True, local_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-pasta",
   "metadata": {},
   "source": [
    "Register our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "great-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "from gym_example.envs.adversarial_v2 import Adversarial_v2\n",
    "\n",
    "select_env = \"adversarial-v2\"\n",
    "register_env(select_env, lambda config: Adversarial_v2())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-anxiety",
   "metadata": {},
   "source": [
    "Next we’ll configure the environment to use proximal policy optimization (PPO) and create an agent to train using RLlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "successful-forge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-21 16:10:32,542\tINFO trainer.py:616 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-04-21 16:10:32,542\tINFO trainer.py:643 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-04-21 16:10:32,599\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-04-21 16:10:33,549\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-04-21 16:10:34,476\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-04-21 16:10:37,290\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "\n",
    "config[\"log_level\"] = \"WARN\"\n",
    "agent = ppo.PPOTrainer(config, env=select_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "complex-omega",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-21 16:21:08,778\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-04-21 16:21:11,259\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "\n",
    "config[\"log_level\"] = \"WARN\"\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 0\n",
    "config[\"log_level\"] = \"WARN\"\n",
    "#config[\"timesteps_per_iteration\"] = 1\n",
    "config[\"train_batch_size\"] = 1\n",
    "config[\"sgd_minibatch_size\"] = 1\n",
    "config[\"num_sgd_iter\"] = 1\n",
    "config[\"timesteps_per_iteration\"] = 4\n",
    "config[\"rollout_fragment_length\"] = 1\n",
    "\n",
    "agent = ppo.PPOTrainer(config, env=select_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-france",
   "metadata": {},
   "source": [
    "For each iteration, we call result = agent.train() to run the episodes, and then call chkpt_file = agent.save(chkpt_root) to save a checkpoint of the latest policy. Then we print metrics that show how well the learning has progressed. The resulting output should look close to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "certified-petroleum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 reward   8.00/ 11.57/ 20.00 len 4.00 saved tmp/exa/checkpoint_7/checkpoint-7\n",
      " 2 reward   8.00/ 12.25/ 20.00 len 4.00 saved tmp/exa/checkpoint_8/checkpoint-8\n",
      " 3 reward   8.00/ 12.22/ 20.00 len 4.00 saved tmp/exa/checkpoint_9/checkpoint-9\n",
      " 4 reward   8.00/ 12.50/ 20.00 len 4.00 saved tmp/exa/checkpoint_10/checkpoint-10\n",
      " 5 reward   8.00/ 13.18/ 20.00 len 4.00 saved tmp/exa/checkpoint_11/checkpoint-11\n",
      " 6 reward   5.00/ 12.50/ 20.00 len 4.00 saved tmp/exa/checkpoint_12/checkpoint-12\n"
     ]
    }
   ],
   "source": [
    "status = \"{:2d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:4.2f} saved {}\"\n",
    "n_iter = 6\n",
    "for n in range(n_iter):\n",
    "    result = agent.train()\n",
    "    chkpt_file = agent.save(chkpt_root)\n",
    "    print(status.format(\n",
    "            n + 1,\n",
    "            result[\"episode_reward_min\"],\n",
    "            result[\"episode_reward_mean\"],\n",
    "            result[\"episode_reward_max\"],\n",
    "            result[\"episode_len_mean\"],\n",
    "            chkpt_file\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "silent-ensemble",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward_max': 20.0,\n",
       " 'episode_reward_min': 5.0,\n",
       " 'episode_reward_mean': 12.5,\n",
       " 'episode_len_mean': 4.0,\n",
       " 'episodes_this_iter': 1,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'custom_metrics': {},\n",
       " 'hist_stats': {'episode_reward': [5.0,\n",
       "   8.0,\n",
       "   9.0,\n",
       "   11.0,\n",
       "   12.0,\n",
       "   20.0,\n",
       "   12.0,\n",
       "   9.0,\n",
       "   17.0,\n",
       "   12.0,\n",
       "   15.0,\n",
       "   20.0],\n",
       "  'episode_lengths': [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]},\n",
       " 'sampler_perf': {'mean_env_wait_ms': 0.6295241681731655,\n",
       "  'mean_raw_obs_processing_ms': 2.5046268024011504,\n",
       "  'mean_inference_ms': 6.261897557364377,\n",
       "  'mean_action_processing_ms': 0.07229448978360566},\n",
       " 'off_policy_estimator': {},\n",
       " 'num_healthy_workers': 0,\n",
       " 'timesteps_total': 48,\n",
       " 'timers': {'sample_time_ms': 4.365,\n",
       "  'sample_throughput': 229.117,\n",
       "  'load_time_ms': 0.644,\n",
       "  'load_throughput': 1552.813,\n",
       "  'learn_time_ms': 2.208,\n",
       "  'learn_throughput': 452.958},\n",
       " 'info': {'learner': {'default_policy': {'cur_kl_coeff': 1.421085492696024e-15,\n",
       "    'cur_lr': 4.999999873689376e-05,\n",
       "    'total_loss': 24.71805,\n",
       "    'policy_loss': 0.0,\n",
       "    'vf_loss': 24.71805,\n",
       "    'vf_explained_var': -1.0,\n",
       "    'kl': 0.0,\n",
       "    'entropy': 5.9914618,\n",
       "    'entropy_coeff': 0.0,\n",
       "    'model': {}}},\n",
       "  'num_steps_sampled': 48,\n",
       "  'num_steps_trained': 48},\n",
       " 'done': False,\n",
       " 'episodes_total': 12,\n",
       " 'training_iteration': 12,\n",
       " 'experiment_id': 'c4aea7d74203482e9a6136aa53825af0',\n",
       " 'date': '2021-04-21_16-21-26',\n",
       " 'timestamp': 1619014886,\n",
       " 'time_this_iter_s': 0.6565890312194824,\n",
       " 'time_total_s': 8.009067058563232,\n",
       " 'pid': 6081,\n",
       " 'hostname': 'paula',\n",
       " 'node_ip': '128.179.144.248',\n",
       " 'config': {'num_workers': 0,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'create_env_on_driver': False,\n",
       "  'rollout_fragment_length': 1,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'num_gpus': 0,\n",
       "  'train_batch_size': 1,\n",
       "  'model': {'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': False,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'num_framestacks': 'auto',\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   'framestack': True},\n",
       "  'optimizer': {},\n",
       "  'gamma': 0.99,\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env_config': {},\n",
       "  'env': 'adversarial-v2',\n",
       "  'normalize_actions': False,\n",
       "  'clip_rewards': None,\n",
       "  'clip_actions': True,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'lr': 5e-05,\n",
       "  'monitor': False,\n",
       "  'log_level': 'WARN',\n",
       "  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'framework': 'tf',\n",
       "  'eager_tracing': False,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_num_episodes': 10,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'sample_async': False,\n",
       "  '_use_trajectory_view_api': True,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'compress_observations': False,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'min_iter_time_s': 0,\n",
       "  'timesteps_per_iteration': 4,\n",
       "  'seed': None,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'memory': 0,\n",
       "  'object_store_memory': 0,\n",
       "  'memory_per_worker': 0,\n",
       "  'object_store_memory_per_worker': 0,\n",
       "  'input': 'sampler',\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {},\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None,\n",
       "   'observation_fn': None,\n",
       "   'replay_mode': 'independent',\n",
       "   'count_steps_by': 'env_steps'},\n",
       "  'logger_config': None,\n",
       "  'replay_sequence_length': 1,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'lambda': 1.0,\n",
       "  'kl_coeff': 0.2,\n",
       "  'sgd_minibatch_size': 1,\n",
       "  'shuffle_sequences': True,\n",
       "  'num_sgd_iter': 1,\n",
       "  'lr_schedule': None,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'grad_clip': None,\n",
       "  'kl_target': 0.01,\n",
       "  'simple_optimizer': False,\n",
       "  '_fake_gpus': False,\n",
       "  'vf_share_layers': -1},\n",
       " 'time_since_restore': 8.009067058563232,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'iterations_since_restore': 12,\n",
       " 'perf': {'cpu_util_percent': 16.7, 'ram_util_percent': 27.4}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir=$HOME/ray_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-agent",
   "metadata": {},
   "source": [
    "# Apply a trained policy in a rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt_root = \"tmp/exa\"\n",
    "chkpt_file = chkpt_root + '/checkpoint_35/checkpoint-35'\n",
    "\n",
    "#restore the latest saved checkpoint for the policy\n",
    "agent.restore(chkpt_file)\n",
    "#create our environment \n",
    "env = gym.make(select_env)\n",
    "#reset its state\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some of the environments created\n",
    "sum_reward = 0\n",
    "n_step = 10\n",
    "for step in range(n_step):\n",
    "    action = agent.compute_action(state)\n",
    "    state, reward, done, info = env.step(action)    \n",
    "    sum_reward += reward\n",
    "    #env.render()    \n",
    "    if done == True:\n",
    "        print(\"cumulative reward\", sum_reward)\n",
    "        visualize(env.image_space) \n",
    "        state = env.reset()\n",
    "        sum_reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-female",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "from gym_example.envs.adversarial_v2 import Adversarial_v2\n",
    "\n",
    "select_env = \"adversarial-v2\"\n",
    "register_env(select_env, lambda config: Adversarial_v2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(ignore_reinit_error=True, local_mode=True)\n",
    "\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "\n",
    "config[\"log_level\"] = \"WARN\"\n",
    "agent = ppo.PPOTrainer(config, env=select_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt_root = \"tmp/exa\"\n",
    "chkpt_file = chkpt_root + '/checkpoint_35/checkpoint-35'\n",
    "\n",
    "#restore the latest saved checkpoint for the policy\n",
    "agent.restore(chkpt_file)\n",
    "#create our environment \n",
    "env = gym.make(select_env)\n",
    "#reset its state\n",
    "state = env.reset()\n",
    "\n",
    "sum_reward = 0\n",
    "n_step = 3\n",
    "for step in range(n_step):\n",
    "    action = agent.compute_action(state)\n",
    "    state, reward, done, info = env.step(action)    \n",
    "    sum_reward += reward\n",
    "    #env.render()    \n",
    "    if done == True:\n",
    "        print(\"cumulative reward\", sum_reward)\n",
    "        visualize(env.image_space) \n",
    "        state = env.reset()\n",
    "        sum_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(env.image_space) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padd with zeroes\n",
    "\n",
    "matrix = np.zeros((40, 40))\n",
    "# actually you can also use result = np.zeros_like(b) \n",
    "# but that also copies the dtype not only the shape\n",
    "\n",
    "matrix[1:39,1:39] = env.image_space\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = matrix2arena(matrix)\n",
    "#my_config.to_proto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize evironment just created\n",
    "\n",
    "try:\n",
    "    environment = AnimalAIEnvironment(\n",
    "            file_name='../env/AnimalAI',\n",
    "            base_port=5007,\n",
    "            arenas_configurations=my_config,\n",
    "            play=True,\n",
    "            worker_id = 5,\n",
    "        )\n",
    "except UnityCommunicationException:\n",
    "    # you'll end up here if you close the environment window directly\n",
    "    # always try to close it from script\n",
    "    environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-lighting",
   "metadata": {},
   "outputs": [],
   "source": [
    "if environment:\n",
    "    environment.close() # takes a few seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-incident",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
