{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc2d0cd9-7250-4c65-a563-09531bd5320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_maze.envs import BasicMultiAgent, AlternateMultiAgent, MazeEnv2, Adversary, AdvPro, AdvPro2, PAIRED\n",
    "from gym_maze.envs.generators import RandomMazeGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import ray\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "import ray.rllib.agents.ppo as ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "697633da-0c14-4fce-901b-9983269fda03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-17 11:44:39,171\tINFO worker.py:665 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "chkpt_root = \"tmp/trial\"\n",
    "\n",
    "shutil.rmtree(chkpt_root, ignore_errors=True, onerror=None)\n",
    "ray_results = \"{}/ray_results/\".format(os.getenv(\"HOME\"))\n",
    "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)\n",
    "\n",
    "ray.init(ignore_reinit_error=True, local_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a958323-6654-4273-9103-d56a72aff6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_episode_adv(env):\n",
    "    '''Given a gym environment (env) it computer one episode and return cumulative reward.\n",
    "    Used to check if our gym environments work'''\n",
    "    \n",
    "    env.reset()\n",
    "    sum_reward = 0\n",
    "    \n",
    "    steps = []\n",
    "    frames = []\n",
    "    \n",
    "    steps.append(env.state.copy())\n",
    "\n",
    "\n",
    "    while not env.done:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        steps.append(env.state.copy())\n",
    "        sum_reward += reward\n",
    "        \n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf293c2-6331-4498-801b-2667f349a597",
   "metadata": {},
   "source": [
    "# AdvPro2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec6dce63-b42b-403c-99c7-1f731e28fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary = Adversary(n_clutter=10, size=8)\n",
    "adversary = run_one_episode_adv(adversary)\n",
    "protagonist = MazeEnv2(adversary.state['image'])\n",
    "adversary = Adversary(n_clutter=10, size=8)\n",
    "\n",
    "policies = {\n",
    "        \"ppo_policy_0\": (ppo.PPOTFPolicy, adversary.observation_space, adversary.action_space, {}),\n",
    "        \"ppo_policy_1\": (ppo.PPOTFPolicy, protagonist.observation_space, protagonist.action_space, {})\n",
    "    }\n",
    "\n",
    "def policy_mapping_fn(agent_id):\n",
    "    if agent_id == 0:\n",
    "        return \"ppo_policy_0\"\n",
    "    else:\n",
    "        return \"ppo_policy_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee238cef-7cb1-4d61-975e-e6016a129691",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_env = 'AdvPro-v1'\n",
    "env_config = AdvPro2(adversary, protagonist, save=True, file_name='advpro2_10_8/', MAX_STEPS=300)\n",
    "register_env(select_env, lambda config: env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84228640-4ff9-4d7d-a95e-da40e6c657c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-17 11:42:01,738\tINFO trainer.py:616 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-05-17 11:42:01,739\tINFO trainer.py:643 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-05-17 11:42:01,824\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-17 11:42:02,897\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-05-17 11:42:04,026\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-05-17 11:42:04,891\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-05-17 11:42:05,903\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-05-17 11:42:06,671\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-05-17 11:42:11,824\tINFO trainable.py:103 -- Trainable.setup took 10.088 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-05-17 11:42:11,825\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "agent = ppo.PPOTrainer(\n",
    "        env=select_env,\n",
    "        config={\n",
    "            \"multiagent\": {\n",
    "                \"policies\": policies,\n",
    "                \"policy_mapping_fn\": policy_mapping_fn,\n",
    "                \"policies_to_train\": [\"ppo_policy_0\", \"ppo_policy_1\"],\n",
    "            },\n",
    "            \"vf_loss_coeff\": 0.01,\n",
    "            \"framework\": \"tf\",\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e258c29e-6e24-458f-be29-764f2a7a19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['num','adv_min','adv_mean','adv_max', \\\n",
    "                            'pro_min','pro_mean','pro_max'], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85216f50-aa51-4b6d-905b-344582414ec0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:928: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "Pro  -120.81    -44.93    0.97\n",
      "Pro  -120.81    -50.66    0.97\n"
     ]
    },
    {
     "ename": "RayTaskError",
     "evalue": "\u001b[36mray::RolloutWorker.par_iter_next()\u001b[39m (pid=4420, ip=128.179.145.215)\n  File \"python/ray/_raylet.pyx\", line 432, in ray._raylet.execute_task.function_executor\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\", line 1152, in par_iter_next\n    return next(self.local_it)\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 327, in gen_rollouts\n    yield self.sample()\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 662, in sample\n    batches = [self.input_reader.next()]\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 95, in next\n    batches = [self.get_data()]\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 224, in get_data\n    item = next(self.rollout_provider)\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 620, in _env_runner\n    sample_collector=sample_collector,\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 1209, in _process_observations_w_trajectory_view_api\n    sample_collector.try_build_truncated_episode_multi_agent_batch()\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/collectors/simple_list_collector.py\", line 756, in try_build_truncated_episode_multi_agent_batch\n    self.postprocess_episode(episode, is_done=False)\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/collectors/simple_list_collector.py\", line 669, in postprocess_episode\n    post_batches[agent_id], other_batches, episode)\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/policy/tf_policy_template.py\", line 246, in postprocess_trajectory\n    episode)\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/postprocessing.py\", line 124, in compute_gae_for_sample_batch\n    sample_batch, index=\"last\")\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/models/modelv2.py\", line 369, in get_input_dict\n    input_dict[view_col] = np.array([data])\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::RolloutWorker.par_iter_next()\u001b[39m (pid=4420, ip=128.179.145.215)\n  File \"python/ray/_raylet.pyx\", line 473, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 497, in ray._raylet.execute_task\nray.exceptions.TaskCancelledError: Task: TaskID(ffffffffffffffffffffffffffffffffffffffff01000000) was cancelled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0027ad98214a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mchkpt_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchkpt_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmin_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'policy_reward_min'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m                         \u001b[0;34m\"continue training without the failed worker, set \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                         \"`'ignore_worker_failures': True`.\")\n\u001b[0;32m--> 526\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# allow logs messages to propagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMAX_WORKER_FAILURE_RETRIES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRayError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ignore_worker_failures\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \"\"\"\n\u001b[1;32m    225\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"step() needs to return a dict.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_exec_impl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;31m# self._iteration gets incremented after this function returns,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_filter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"LocalIterator[T]\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_filter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"LocalIterator[T]\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_flatten\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"LocalIterator[T[0]]\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36madd_wait_hooks\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    826\u001b[0m                             \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_fetch_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                         \u001b[0mnew_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                         \u001b[0mnew_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mbase_iterator\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mactive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m                     \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpar_iter_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactive\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                     \u001b[0;31m# Always yield after each round of gets with timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclient_mode_enabled\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_client_hook_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1454\u001b[0m                     \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_object_store_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayTaskError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1456\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_instanceof_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1457\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRayTaskError\u001b[0m: \u001b[36mray::RolloutWorker.par_iter_next()\u001b[39m (pid=4420, ip=128.179.145.215)\n  File \"python/ray/_raylet.pyx\", line 432, in ray._raylet.execute_task.function_executor\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\", line 1152, in par_iter_next\n    return next(self.local_it)\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 327, in gen_rollouts\n    yield self.sample()\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 662, in sample\n    batches = [self.input_reader.next()]\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 95, in next\n    batches = [self.get_data()]\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 224, in get_data\n    item = next(self.rollout_provider)\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 620, in _env_runner\n    sample_collector=sample_collector,\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 1209, in _process_observations_w_trajectory_view_api\n    sample_collector.try_build_truncated_episode_multi_agent_batch()\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/collectors/simple_list_collector.py\", line 756, in try_build_truncated_episode_multi_agent_batch\n    self.postprocess_episode(episode, is_done=False)\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/collectors/simple_list_collector.py\", line 669, in postprocess_episode\n    post_batches[agent_id], other_batches, episode)\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/policy/tf_policy_template.py\", line 246, in postprocess_trajectory\n    episode)\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/evaluation/postprocessing.py\", line 124, in compute_gae_for_sample_batch\n    sample_batch, index=\"last\")\n  File \"/home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/models/modelv2.py\", line 369, in get_input_dict\n    input_dict[view_col] = np.array([data])\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::RolloutWorker.par_iter_next()\u001b[39m (pid=4420, ip=128.179.145.215)\n  File \"python/ray/_raylet.pyx\", line 473, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 497, in ray._raylet.execute_task\nray.exceptions.TaskCancelledError: Task: TaskID(ffffffffffffffffffffffffffffffffffffffff01000000) was cancelled"
     ]
    }
   ],
   "source": [
    "status = \"{:2d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:4.2f} saved {}\"\n",
    "n_iter = 1000\n",
    "for n in range(n_iter):\n",
    "    result = agent.train()\n",
    "    chkpt_file = agent.save(chkpt_root)\n",
    "    min_reward = result['policy_reward_min']\n",
    "    mean_reward  = result['policy_reward_mean']\n",
    "    max_reward = result['policy_reward_max']\n",
    "    id0 = \"ppo_policy_0\"\n",
    "    id1 = \"ppo_policy_1\"\n",
    "    #print(n + 1, 'Agent -  min - mean - max ')\n",
    "    #print('-----------------------------------------------------')\n",
    "    #print('Adv ', round(min_reward[id0], 2), '  ', round(mean_reward[id0], 2), '  ', round(max_reward[id0], 2))\n",
    "    print('Pro ', round(min_reward[id1], 2), '  ', round(mean_reward[id1], 2), '  ', round(max_reward[id1], 2))\n",
    "    #print(' ')\n",
    "    df = df.append({'num': n+1,'adv_min': min_reward[id0],'adv_mean': mean_reward[id0],'adv_max': max_reward[id0], \\\n",
    "                            'pro_min': min_reward[id1],'pro_mean': mean_reward[id1],'pro_max': max_reward[id1]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be3f26-fc95-4f38-b43c-a91f0cd83c52",
   "metadata": {},
   "source": [
    "# PAIRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "370c9e06-1bb0-462e-b646-b619f06c1065",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary = Adversary(n_clutter=10, size=8)\n",
    "adversary = run_one_episode_adv(adversary)\n",
    "protagonist = MazeEnv2(adversary.state['image'])\n",
    "antagonist = MazeEnv2(adversary.state['image'])\n",
    "adversary = Adversary(n_clutter=10, size=8)\n",
    "\n",
    "policies = {\n",
    "        \"ppo_policy_0\": (ppo.PPOTFPolicy, adversary.observation_space, adversary.action_space, {}),\n",
    "        \"ppo_policy_1\": (ppo.PPOTFPolicy, protagonist.observation_space, protagonist.action_space, {}),\n",
    "        \"ppo_policy_2\": (ppo.PPOTFPolicy, antagonist.observation_space, antagonist.action_space, {})\n",
    "    }\n",
    "\n",
    "def policy_mapping_fn(agent_id):\n",
    "    if agent_id == 0:\n",
    "        return \"ppo_policy_0\"\n",
    "    elif agent_id == 1:\n",
    "        return \"ppo_policy_1\"\n",
    "    elif agent_id == 2:\n",
    "        return \"ppo_policy_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb74241-5bbe-4d3a-9d4a-2545ab3dc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_env = 'Paired-v0'\n",
    "env_config = PAIRED(adversary, protagonist, antagonist, save=True, \\\n",
    "                    file_name='paired_pos_10_8/', MAX_STEPS=300)\n",
    "register_env(select_env, lambda config: env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08fba8c4-2c2d-4c94-bb95-988f4082f28c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:39:24,386\tINFO trainer.py:616 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-05-13 11:39:24,387\tINFO trainer.py:643 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-05-13 11:39:24,463\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:39:25,311\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-05-13 11:39:26,433\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-05-13 11:39:27,885\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-05-13 11:39:28,627\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-05-13 11:39:29,592\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-05-13 11:39:30,844\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-05-13 11:39:31,739\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-05-13 11:39:32,709\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-05-13 11:39:41,477\tINFO trainable.py:103 -- Trainable.setup took 17.094 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-05-13 11:39:41,478\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "agent = ppo.PPOTrainer(\n",
    "        env=select_env,\n",
    "        config={\n",
    "            \"multiagent\": {\n",
    "                \"policies\": policies,\n",
    "                \"policy_mapping_fn\": policy_mapping_fn,\n",
    "                \"policies_to_train\": [\"ppo_policy_0\", \"ppo_policy_1\", \"ppo_policy_2\"],\n",
    "            },\n",
    "            \"vf_loss_coeff\": 0.01,\n",
    "            \"framework\": \"tf\",\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e373b-6d73-4d8e-bc62-68197a3add94",
   "metadata": {},
   "source": [
    "## 1. Uncoupled PAIRED: reward of the protagonist and antagonist independent of reward of adversary (shortest path length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e3fe3e7-35cc-4d65-addd-b23513b96f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['num','adv_min','adv_mean','adv_max', \\\n",
    "                            'pro_min','pro_mean','pro_max', \\\n",
    "                            'ant_min','ant_mean','ant_max'], dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc604158-283b-496a-b456-0299c95699eb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/psanchezlopez/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:928: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "1 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.0    3.29    7.0\n",
      "Pro  -139.64    -34.79    -0.08\n",
      "Ant  -79.91    -28.64    0.91\n",
      " \n",
      "2 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    -0.73    7.0\n",
      "Pro  -500.0    -56.84    0.97\n",
      "Ant  -500.0    -66.0    0.95\n",
      " \n",
      "3 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    0.08    7.0\n",
      "Pro  -500.0    -64.92    0.97\n",
      "Ant  -500.0    -65.97    0.95\n",
      " \n",
      "4 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    1.15    7.0\n",
      "Pro  -500.0    -64.91    1.0\n",
      "Ant  -500.0    -58.42    1.0\n",
      " \n",
      "5 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    1.26    7.0\n",
      "Pro  -500.0    -74.46    1.0\n",
      "Ant  -500.0    -61.0    1.0\n",
      " \n",
      "6 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    1.45    7.0\n",
      "Pro  -500.0    -72.91    1.0\n",
      "Ant  -500.0    -62.39    1.0\n",
      " \n",
      "7 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    1.55    7.0\n",
      "Pro  -500.0    -77.77    1.0\n",
      "Ant  -500.0    -64.32    1.0\n",
      " \n",
      "8 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    1.62    7.0\n",
      "Pro  -500.0    -79.97    1.0\n",
      "Ant  -500.0    -66.96    1.0\n",
      " \n",
      "9 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    1.6    7.0\n",
      "Pro  -500.0    -83.64    1.0\n",
      "Ant  -500.0    -73.03    1.0\n",
      " \n",
      "10 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    1.42    7.0\n",
      "Pro  -500.0    -86.49    1.0\n",
      "Ant  -500.0    -82.8    1.0\n",
      " \n",
      "11 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -5.0    2.22    7.0\n",
      "Pro  -310.91    -89.03    1.0\n",
      "Ant  -316.85    -80.67    1.0\n",
      " \n",
      "12 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -5.0    2.15    7.0\n",
      "Pro  -310.91    -93.07    1.0\n",
      "Ant  -316.85    -88.51    1.0\n",
      " \n",
      "13 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -5.0    2.1    7.0\n",
      "Pro  -310.91    -94.64    1.0\n",
      "Ant  -316.85    -93.32    1.0\n",
      " \n",
      "14 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -5.0    1.96    7.0\n",
      "Pro  -310.91    -98.98    1.0\n",
      "Ant  -316.85    -111.82    1.0\n",
      " \n",
      "15 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -5.0    1.9    7.0\n",
      "Pro  -310.91    -96.77    0.99\n",
      "Ant  -326.75    -120.01    1.0\n",
      " \n",
      "16 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -5.0    1.93    7.0\n",
      "Pro  -310.91    -101.52    0.99\n",
      "Ant  -326.75    -122.93    1.0\n",
      " \n",
      "17 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -5.0    1.88    7.0\n",
      "Pro  -310.91    -103.96    0.97\n",
      "Ant  -326.75    -122.93    1.0\n",
      " \n",
      "18 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -5.0    1.85    7.0\n",
      "Pro  -310.91    -104.8    0.97\n",
      "Ant  -326.75    -125.68    1.0\n",
      " \n",
      "19 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -5.0    2.1    7.0\n",
      "Pro  -363.38    -108.98    0.97\n",
      "Ant  -337.64    -126.39    1.0\n",
      " \n",
      "20 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -5.0    2.17    7.0\n",
      "Pro  -363.38    -116.12    0.97\n",
      "Ant  -337.64    -123.42    1.0\n",
      " \n",
      "21 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    1.89    7.0\n",
      "Pro  -363.38    -109.14    1.0\n",
      "Ant  -337.64    -123.61    1.0\n",
      " \n",
      "22 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    2.12    7.0\n",
      "Pro  -363.38    -111.28    1.0\n",
      "Ant  -337.64    -117.3    1.0\n",
      " \n",
      "23 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    1.97    7.0\n",
      "Pro  -363.38    -122.2    1.0\n",
      "Ant  -337.64    -113.19    1.0\n",
      " \n",
      "24 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    2.04    7.0\n",
      "Pro  -363.38    -124.32    1.0\n",
      "Ant  -337.64    -105.88    1.0\n",
      " \n",
      "25 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    1.66    7.0\n",
      "Pro  -363.38    -126.26    1.0\n",
      "Ant  -337.64    -105.23    1.0\n",
      " \n",
      "26 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    1.68    7.0\n",
      "Pro  -396.05    -130.62    1.0\n",
      "Ant  -345.56    -105.79    1.0\n",
      " \n",
      "27 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    1.83    7.0\n",
      "Pro  -396.05    -125.41    1.0\n",
      "Ant  -345.56    -107.81    1.0\n",
      " \n",
      "28 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    1.85    7.0\n",
      "Pro  -396.05    -128.39    1.0\n",
      "Ant  -345.56    -107.5    1.0\n",
      " \n",
      "29 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    1.82    7.0\n",
      "Pro  -396.05    -126.2    1.0\n",
      "Ant  -356.45    -111.4    1.0\n",
      " \n",
      "30 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    1.74    7.0\n",
      "Pro  -396.05    -121.18    1.0\n",
      "Ant  -415.85    -117.46    1.0\n",
      " \n",
      "31 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    1.84    7.0\n",
      "Pro  -396.05    -122.07    1.0\n",
      "Ant  -415.85    -119.71    1.0\n",
      " \n",
      "32 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    2.24    7.0\n",
      "Pro  -396.05    -124.81    1.0\n",
      "Ant  -415.85    -123.31    1.0\n",
      " \n",
      "33 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    2.32    7.0\n",
      "Pro  -396.05    -127.12    1.0\n",
      "Ant  -415.85    -131.17    1.0\n",
      " \n",
      "34 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    2.48    7.0\n",
      "Pro  -396.05    -123.83    1.0\n",
      "Ant  -415.85    -135.98    1.0\n",
      " \n",
      "35 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.0    2.51    7.0\n",
      "Pro  -396.05    -126.23    1.0\n",
      "Ant  -415.85    -135.34    1.0\n",
      " \n",
      "36 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -5.0    3.02    7.0\n",
      "Pro  -471.29    -131.37    1.0\n",
      "Ant  -415.85    -139.29    1.0\n",
      " \n",
      "37 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -5.0    2.9    7.0\n",
      "Pro  -471.29    -131.56    1.0\n",
      "Ant  -415.85    -145.58    1.0\n",
      " \n",
      "38 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.86    7.0\n",
      "Pro  -471.29    -133.75    1.0\n",
      "Ant  -415.85    -144.29    1.0\n",
      " \n",
      "39 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.76    7.0\n",
      "Pro  -473.27    -148.33    1.0\n",
      "Ant  -415.85    -153.21    1.0\n",
      " \n",
      "40 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.76    7.0\n",
      "Pro  -473.27    -151.12    1.0\n",
      "Ant  -447.53    -157.57    1.0\n",
      " \n",
      "41 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.9    7.0\n",
      "Pro  -473.27    -155.52    1.0\n",
      "Ant  -447.53    -152.92    1.0\n",
      " \n",
      "42 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.85    7.0\n",
      "Pro  -473.27    -160.14    1.0\n",
      "Ant  -447.53    -150.12    1.0\n",
      " \n",
      "43 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.8    7.0\n",
      "Pro  -473.27    -162.88    1.0\n",
      "Ant  -471.29    -155.76    0.99\n",
      " \n",
      "44 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.82    7.0\n",
      "Pro  -473.27    -161.8    1.0\n",
      "Ant  -471.29    -151.53    0.99\n",
      " \n",
      "45 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.68    7.0\n",
      "Pro  -484.16    -168.4    1.0\n",
      "Ant  -471.29    -148.76    1.0\n",
      " \n",
      "46 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.56    7.0\n",
      "Pro  -485.15    -176.58    1.0\n",
      "Ant  -471.29    -154.8    1.0\n",
      " \n",
      "47 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.6    7.0\n",
      "Pro  -485.15    -176.89    1.0\n",
      "Ant  -477.23    -157.96    1.0\n",
      " \n",
      "48 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.54    7.0\n",
      "Pro  -485.15    -179.07    1.0\n",
      "Ant  -477.23    -152.44    1.0\n",
      " \n",
      "49 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.62    7.0\n",
      "Pro  -485.15    -176.94    1.0\n",
      "Ant  -477.23    -157.0    1.0\n",
      " \n",
      "50 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.74    7.0\n",
      "Pro  -485.15    -174.12    1.0\n",
      "Ant  -477.23    -157.99    1.0\n",
      " \n",
      "51 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.81    7.0\n",
      "Pro  -485.15    -162.95    1.0\n",
      "Ant  -477.23    -155.08    1.0\n",
      " \n",
      "52 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.73    7.0\n",
      "Pro  -485.15    -155.67    1.0\n",
      "Ant  -477.23    -154.5    1.0\n",
      " \n",
      "53 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.6    7.0\n",
      "Pro  -485.15    -149.64    1.0\n",
      "Ant  -477.23    -149.42    1.0\n",
      " \n",
      "54 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.55    7.0\n",
      "Pro  -485.15    -153.5    1.0\n",
      "Ant  -477.23    -150.29    1.0\n",
      " \n",
      "55 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.0    2.63    7.0\n",
      "Pro  -485.15    -150.51    1.0\n",
      "Ant  -477.23    -150.75    1.0\n",
      " \n",
      "56 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -3.0    2.67    7.0\n",
      "Pro  -485.15    -156.73    1.0\n",
      "Ant  -477.23    -156.72    1.0\n",
      " \n",
      "57 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -3.0    2.67    7.0\n",
      "Pro  -485.15    -158.08    1.0\n",
      "Ant  -477.23    -161.75    1.0\n",
      " \n",
      "58 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -3.0    2.67    7.0\n",
      "Pro  -483.17    -153.53    1.0\n",
      "Ant  -486.14    -156.97    1.0\n",
      " \n",
      "59 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -3.0    2.74    7.0\n",
      "Pro  -483.17    -159.77    0.99\n",
      "Ant  -486.14    -152.02    1.0\n",
      " \n",
      "60 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -3.0    2.7    7.0\n",
      "Pro  -469.31    -154.31    0.99\n",
      "Ant  -486.14    -155.8    1.0\n",
      " \n"
     ]
    }
   ],
   "source": [
    "status = \"{:2d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:4.2f} saved {}\"\n",
    "n_iter = 60\n",
    "for n in range(n_iter):\n",
    "    result = agent.train()\n",
    "    chkpt_file = agent.save(chkpt_root)\n",
    "    min_reward = result['policy_reward_min']\n",
    "    mean_reward  = result['policy_reward_mean']\n",
    "    max_reward = result['policy_reward_max']\n",
    "    id0 = \"ppo_policy_0\"\n",
    "    id1 = \"ppo_policy_1\"\n",
    "    id2 = \"ppo_policy_2\"\n",
    "    print(n + 1, 'Agent -  min - mean - max ')\n",
    "    print('-----------------------------------------------------')\n",
    "    print('Adv ', round(min_reward[id0], 2), '  ', round(mean_reward[id0], 2), '  ', round(max_reward[id0], 2))\n",
    "    print('Pro ', round(min_reward[id1], 2), '  ', round(mean_reward[id1], 2), '  ', round(max_reward[id1], 2))\n",
    "    print('Ant ', round(min_reward[id2], 2), '  ', round(mean_reward[id2], 2), '  ', round(max_reward[id2], 2))\n",
    "    print(' ')\n",
    "    \n",
    "    df = df.append({'num': n+1,'adv_min': min_reward[id0],'adv_mean': mean_reward[id0],'adv_max': max_reward[id0], \\\n",
    "                            'pro_min': min_reward[id1],'pro_mean': mean_reward[id1],'pro_max': max_reward[id1], \\\n",
    "                            'ant_min': min_reward[id2],'ant_mean': mean_reward[id2],'ant_max': max_reward[id2]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb93e3f-8753-4143-b923-7a9f24a96995",
   "metadata": {},
   "source": [
    "## 2. Positive PAIRED: reward adversary sum of reward of the protagonist and antagonist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dce4f153-525c-4e63-bba5-fd53186d61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['num','adv_min','adv_mean','adv_max', \\\n",
    "                            'pro_min','pro_mean','pro_max', \\\n",
    "                            'ant_min','ant_mean','ant_max'], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ce4ba55-0fbb-4603-b66e-0d92f9ff662e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d69507bc93cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mchkpt_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchkpt_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmin_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'policy_reward_min'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# allow logs messages to propagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMAX_WORKER_FAILURE_RETRIES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRayError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ignore_worker_failures\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \"\"\"\n\u001b[1;32m    225\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"step() needs to return a dict.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_exec_impl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;31m# self._iteration gets incremented after this function returns,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animalai/lib/python3.7/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "status = \"{:2d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:4.2f} saved {}\"\n",
    "n_iter = 500\n",
    "for n in range(n_iter):\n",
    "    result = agent.train()\n",
    "    chkpt_file = agent.save(chkpt_root)\n",
    "    min_reward = result['policy_reward_min']\n",
    "    mean_reward  = result['policy_reward_mean']\n",
    "    max_reward = result['policy_reward_max']\n",
    "    id0 = \"ppo_policy_0\"\n",
    "    id1 = \"ppo_policy_1\"\n",
    "    id2 = \"ppo_policy_2\"\n",
    "    print(n + 1, 'Agent -  min - mean - max ')\n",
    "    print('-----------------------------------------------------')\n",
    "    print('Adv ', round(min_reward[id0], 2), '  ', round(mean_reward[id0], 2), '  ', round(max_reward[id0], 2))\n",
    "    print('Pro ', round(min_reward[id1], 2), '  ', round(mean_reward[id1], 2), '  ', round(max_reward[id1], 2))\n",
    "    print('Ant ', round(min_reward[id2], 2), '  ', round(mean_reward[id2], 2), '  ', round(max_reward[id2], 2))\n",
    "    print(' ')\n",
    "    \n",
    "    df = df.append({'num': n+1,'adv_min': min_reward[id0],'adv_mean': mean_reward[id0],'adv_max': max_reward[id0], \\\n",
    "                            'pro_min': min_reward[id1],'pro_mean': mean_reward[id1],'pro_max': max_reward[id1], \\\n",
    "                            'ant_min': min_reward[id2],'ant_mean': mean_reward[id2],'ant_max': max_reward[id2]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "099a0f6c-53da-4f10-9cec-9176db0c5e16",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -247.42    2.0\n",
      "Pro  -299.01    -105.96    1.0\n",
      "Ant  -299.01    -141.46    1.0\n",
      " \n",
      "2 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -244.02    2.0\n",
      "Pro  -299.01    -110.55    1.0\n",
      "Ant  -299.01    -133.47    1.0\n",
      " \n",
      "3 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -239.08    2.0\n",
      "Pro  -299.01    -106.71    1.0\n",
      "Ant  -299.01    -132.37    1.0\n",
      " \n",
      "4 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -232.0    2.0\n",
      "Pro  -299.01    -106.22    1.0\n",
      "Ant  -299.01    -125.78    1.0\n",
      " \n",
      "5 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -231.18    2.0\n",
      "Pro  -299.01    -105.27    1.0\n",
      "Ant  -299.01    -125.91    1.0\n",
      " \n",
      "6 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -228.94    2.0\n",
      "Pro  -299.01    -104.71    1.0\n",
      "Ant  -299.01    -124.23    1.0\n",
      " \n",
      "7 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -236.34    2.0\n",
      "Pro  -299.01    -112.95    1.0\n",
      "Ant  -300.0    -123.39    1.0\n",
      " \n",
      "8 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -219.17    2.0\n",
      "Pro  -299.01    -98.59    1.0\n",
      "Ant  -300.0    -120.58    1.0\n",
      " \n",
      "9 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -217.91    2.0\n",
      "Pro  -299.01    -96.98    1.0\n",
      "Ant  -300.0    -120.93    1.0\n",
      " \n",
      "10 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -224.2    2.0\n",
      "Pro  -299.01    -107.32    1.0\n",
      "Ant  -300.0    -116.89    1.0\n",
      " \n",
      "11 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -215.3    2.0\n",
      "Pro  -299.01    -106.42    1.0\n",
      "Ant  -300.0    -108.88    1.0\n",
      " \n",
      "12 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -238.65    1.98\n",
      "Pro  -299.01    -121.1    1.0\n",
      "Ant  -300.0    -117.55    0.99\n",
      " \n",
      "13 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -237.77    1.98\n",
      "Pro  -299.01    -120.83    1.0\n",
      "Ant  -300.0    -116.94    0.99\n",
      " \n",
      "14 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -222.33    1.98\n",
      "Pro  -299.01    -109.3    1.0\n",
      "Ant  -299.01    -113.03    0.99\n",
      " \n",
      "15 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -229.99    1.98\n",
      "Pro  -299.01    -120.96    1.0\n",
      "Ant  -299.01    -109.03    0.99\n",
      " \n",
      "16 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -246.03    1.98\n",
      "Pro  -299.01    -129.07    1.0\n",
      "Ant  -299.01    -116.96    0.99\n",
      " \n",
      "17 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -589.11    -239.73    0.99\n",
      "Pro  -297.03    -119.85    1.0\n",
      "Ant  -299.01    -119.88    0.99\n",
      " \n",
      "18 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -590.1    -237.7    -1.08\n",
      "Pro  -297.03    -121.25    1.0\n",
      "Ant  -299.01    -116.46    0.99\n",
      " \n",
      "19 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -590.1    -233.12    -1.08\n",
      "Pro  -297.03    -118.34    1.0\n",
      "Ant  -299.01    -114.78    0.99\n",
      " \n",
      "20 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -225.45    -1.08\n",
      "Pro  -299.01    -107.02    1.0\n",
      "Ant  -299.01    -118.43    0.99\n",
      " \n",
      "21 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -227.12    -0.02\n",
      "Pro  -299.01    -111.77    1.0\n",
      "Ant  -299.01    -115.35    0.99\n",
      " \n",
      "22 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -216.03    -0.02\n",
      "Pro  -299.01    -107.81    1.0\n",
      "Ant  -299.01    -108.22    0.99\n",
      " \n",
      "23 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -216.18    -0.02\n",
      "Pro  -299.01    -105.07    0.98\n",
      "Ant  -299.01    -111.12    0.99\n",
      " \n",
      "24 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -215.84    -0.02\n",
      "Pro  -299.01    -103.25    1.0\n",
      "Ant  -299.01    -112.59    0.99\n",
      " \n",
      "25 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -225.3    -0.02\n",
      "Pro  -299.01    -109.54    1.0\n",
      "Ant  -299.01    -115.77    0.99\n",
      " \n",
      "26 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -237.22    -0.02\n",
      "Pro  -299.01    -114.59    1.0\n",
      "Ant  -299.01    -122.62    0.99\n",
      " \n",
      "27 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -240.86    1.94\n",
      "Pro  -299.01    -118.55    1.0\n",
      "Ant  -299.01    -122.31    1.0\n",
      " \n",
      "28 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -242.71    1.94\n",
      "Pro  -299.01    -122.57    1.0\n",
      "Ant  -299.01    -120.14    1.0\n",
      " \n",
      "29 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -592.08    -237.1    1.97\n",
      "Pro  -299.01    -119.36    1.0\n",
      "Ant  -299.01    -117.74    1.0\n",
      " \n",
      "30 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -592.08    -246.41    1.97\n",
      "Pro  -299.01    -131.88    1.0\n",
      "Ant  -299.01    -114.54    1.0\n",
      " \n",
      "31 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -592.08    -237.55    1.97\n",
      "Pro  -299.01    -130.89    1.0\n",
      "Ant  -299.01    -106.66    1.0\n",
      " \n",
      "32 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -592.08    -231.11    1.97\n",
      "Pro  -299.01    -129.86    1.0\n",
      "Ant  -299.01    -101.26    1.0\n",
      " \n",
      "33 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -592.08    -208.13    1.97\n",
      "Pro  -299.01    -118.8    1.0\n",
      "Ant  -299.01    -89.33    1.0\n",
      " \n",
      "34 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -222.51    1.97\n",
      "Pro  -299.01    -123.5    1.0\n",
      "Ant  -299.01    -99.01    1.0\n",
      " \n",
      "35 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -220.42    1.97\n",
      "Pro  -299.01    -122.14    1.0\n",
      "Ant  -299.01    -98.28    1.0\n",
      " \n",
      "36 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -218.27    1.97\n",
      "Pro  -299.01    -118.02    0.98\n",
      "Ant  -299.01    -100.25    1.0\n",
      " \n",
      "37 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -235.71    1.68\n",
      "Pro  -299.01    -126.39    0.98\n",
      "Ant  -299.01    -109.32    0.96\n",
      " \n",
      "38 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -220.54    1.68\n",
      "Pro  -299.01    -111.84    0.98\n",
      "Ant  -299.01    -108.7    0.96\n",
      " \n",
      "39 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -228.46    1.68\n",
      "Pro  -299.01    -112.63    0.98\n",
      "Ant  -299.01    -115.83    0.97\n",
      " \n",
      "40 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -235.73    -2.02\n",
      "Pro  -299.01    -115.85    0.98\n",
      "Ant  -299.01    -119.88    0.99\n",
      " \n",
      "41 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -241.07    1.36\n",
      "Pro  -299.01    -118.52    0.97\n",
      "Ant  -299.01    -122.56    0.99\n",
      " \n",
      "42 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -588.12    -224.99    1.36\n",
      "Pro  -298.02    -108.54    0.97\n",
      "Ant  -297.03    -116.45    0.99\n",
      " \n",
      "43 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -588.12    -221.37    1.36\n",
      "Pro  -298.02    -106.67    0.97\n",
      "Ant  -297.03    -114.7    0.99\n",
      " \n",
      "44 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -588.12    -229.17    1.36\n",
      "Pro  -298.02    -108.9    0.99\n",
      "Ant  -297.03    -120.27    0.99\n",
      " \n",
      "45 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -585.15    -230.99    1.36\n",
      "Pro  -299.01    -113.51    0.99\n",
      "Ant  -297.03    -117.48    0.99\n",
      " \n",
      "46 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -227.75    1.36\n",
      "Pro  -299.01    -113.31    0.99\n",
      "Ant  -298.02    -114.44    0.99\n",
      " \n",
      "47 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -225.42    1.36\n",
      "Pro  -299.01    -108.75    0.99\n",
      "Ant  -298.02    -116.67    0.99\n",
      " \n",
      "48 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -228.22    1.36\n",
      "Pro  -299.01    -107.12    0.99\n",
      "Ant  -298.02    -121.1    0.98\n",
      " \n",
      "49 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -225.8    2.0\n",
      "Pro  -299.01    -107.79    1.0\n",
      "Ant  -298.02    -118.01    1.0\n",
      " \n",
      "50 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -228.97    2.0\n",
      "Pro  -299.01    -111.22    1.0\n",
      "Ant  -298.02    -117.75    1.0\n",
      " \n",
      "51 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -242.26    2.0\n",
      "Pro  -299.01    -118.54    1.0\n",
      "Ant  -298.02    -123.73    1.0\n",
      " \n",
      "52 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -223.33    2.0\n",
      "Pro  -299.01    -113.4    1.0\n",
      "Ant  -298.02    -109.93    1.0\n",
      " \n",
      "53 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -221.05    2.0\n",
      "Pro  -299.01    -111.74    1.0\n",
      "Ant  -298.02    -109.31    1.0\n",
      " \n",
      "54 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -212.6    2.0\n",
      "Pro  -299.01    -111.57    1.0\n",
      "Ant  -297.03    -101.03    1.0\n",
      " \n",
      "55 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -230.0    2.0\n",
      "Pro  -299.01    -127.39    1.0\n",
      "Ant  -297.03    -102.61    1.0\n",
      " \n",
      "56 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -596.04    -232.5    1.96\n",
      "Pro  -299.01    -129.03    1.0\n",
      "Ant  -298.02    -103.47    0.98\n",
      " \n",
      "57 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -589.11    -216.55    1.98\n",
      "Pro  -299.01    -123.64    1.0\n",
      "Ant  -298.02    -92.91    1.0\n",
      " \n",
      "58 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -584.16    -207.97    1.98\n",
      "Pro  -299.01    -121.53    1.0\n",
      "Ant  -298.02    -86.43    1.0\n",
      " \n",
      "59 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -584.16    -205.24    1.98\n",
      "Pro  -299.01    -116.81    1.0\n",
      "Ant  -298.02    -88.43    1.0\n",
      " \n",
      "60 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -584.16    -205.44    1.98\n",
      "Pro  -299.01    -108.82    1.0\n",
      "Ant  -298.02    -96.61    1.0\n",
      " \n",
      "61 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -584.16    -207.7    1.98\n",
      "Pro  -299.01    -107.24    1.0\n",
      "Ant  -298.02    -100.47    1.0\n",
      " \n",
      "62 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -584.16    -196.16    1.98\n",
      "Pro  -297.03    -99.13    1.0\n",
      "Ant  -298.02    -97.03    1.0\n",
      " \n",
      "63 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -584.16    -177.58    1.98\n",
      "Pro  -297.03    -91.63    1.0\n",
      "Ant  -297.03    -85.96    1.0\n",
      " \n",
      "64 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -582.18    -176.77    1.98\n",
      "Pro  -296.04    -84.35    1.0\n",
      "Ant  -297.03    -92.42    0.99\n",
      " \n",
      "65 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -582.18    -185.15    1.98\n",
      "Pro  -296.04    -83.01    1.0\n",
      "Ant  -297.03    -102.14    0.99\n",
      " \n",
      "66 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -582.18    -176.45    1.98\n",
      "Pro  -296.04    -81.66    1.0\n",
      "Ant  -297.03    -94.79    0.99\n",
      " \n",
      "67 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -528.72    -168.57    1.98\n",
      "Pro  -296.04    -76.76    1.0\n",
      "Ant  -297.03    -91.81    0.99\n",
      " \n",
      "68 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -528.72    -170.31    1.98\n",
      "Pro  -296.04    -81.93    1.0\n",
      "Ant  -297.03    -88.38    0.99\n",
      " \n",
      "69 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -520.8    -159.11    1.98\n",
      "Pro  -298.02    -77.09    1.0\n",
      "Ant  -297.03    -82.02    0.99\n",
      " \n",
      "70 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -575.15    -156.26    1.94\n",
      "Pro  -298.02    -74.58    1.0\n",
      "Ant  -296.04    -81.68    0.99\n",
      " \n",
      "71 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -584.16    -145.83    1.94\n",
      "Pro  -298.02    -67.95    1.0\n",
      "Ant  -296.04    -77.88    0.99\n",
      " \n",
      "72 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -584.16    -158.24    0.47\n",
      "Pro  -300.0    -77.15    1.0\n",
      "Ant  -296.04    -81.09    0.99\n",
      " \n",
      "73 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -584.16    -151.06    0.88\n",
      "Pro  -300.0    -75.3    1.0\n",
      "Ant  -296.04    -75.76    1.0\n",
      " \n",
      "74 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -584.16    -157.28    0.88\n",
      "Pro  -300.0    -78.5    1.0\n",
      "Ant  -296.04    -78.78    1.0\n",
      " \n",
      "75 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -584.16    -154.6    0.88\n",
      "Pro  -300.0    -82.1    1.0\n",
      "Ant  -296.04    -72.5    1.0\n",
      " \n",
      "76 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -584.16    -150.79    0.88\n",
      "Pro  -300.0    -78.3    1.0\n",
      "Ant  -296.04    -72.49    1.0\n",
      " \n",
      "77 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -584.16    -161.15    0.88\n",
      "Pro  -300.0    -81.16    1.0\n",
      "Ant  -299.01    -79.99    1.0\n",
      " \n",
      "78 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -584.16    -162.73    0.88\n",
      "Pro  -300.0    -80.94    1.0\n",
      "Ant  -299.01    -81.79    1.0\n",
      " \n",
      "79 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -567.33    -159.49    0.88\n",
      "Pro  -298.02    -79.53    1.0\n",
      "Ant  -299.01    -79.96    1.0\n",
      " \n",
      "80 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -570.3    -155.36    0.88\n",
      "Pro  -298.02    -78.32    1.0\n",
      "Ant  -299.01    -77.04    1.0\n",
      " \n",
      "81 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -570.3    -157.11    -0.99\n",
      "Pro  -298.02    -75.76    1.0\n",
      "Ant  -299.01    -81.35    0.99\n",
      " \n",
      "82 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -570.3    -165.95    -0.99\n",
      "Pro  -298.02    -81.33    1.0\n",
      "Ant  -299.01    -84.62    0.99\n",
      " \n",
      "83 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -570.3    -162.29    -0.99\n",
      "Pro  -298.02    -78.13    1.0\n",
      "Ant  -299.01    -84.17    0.99\n",
      " \n",
      "84 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -570.3    -162.8    -0.99\n",
      "Pro  -297.03    -76.9    1.0\n",
      "Ant  -299.01    -85.9    0.99\n",
      " \n",
      "85 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -570.3    -175.46    -0.99\n",
      "Pro  -297.03    -90.16    1.0\n",
      "Ant  -298.02    -85.3    0.99\n",
      " \n",
      "86 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -570.3    -178.69    -0.99\n",
      "Pro  -297.03    -99.24    1.0\n",
      "Ant  -298.02    -79.44    0.99\n",
      " \n",
      "87 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -570.3    -186.61    -0.99\n",
      "Pro  -297.03    -97.16    1.0\n",
      "Ant  -298.02    -89.45    0.97\n",
      " \n",
      "88 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -590.1    -196.61    1.98\n",
      "Pro  -297.03    -101.74    1.0\n",
      "Ant  -298.02    -94.87    0.99\n",
      " \n",
      "89 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -590.1    -190.09    1.98\n",
      "Pro  -297.03    -99.85    1.0\n",
      "Ant  -298.02    -90.23    0.99\n",
      " \n",
      "90 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -590.1    -189.88    1.98\n",
      "Pro  -296.04    -96.15    1.0\n",
      "Ant  -298.02    -93.73    0.99\n",
      " \n",
      "91 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -594.06    -199.53    1.98\n",
      "Pro  -296.04    -96.79    1.0\n",
      "Ant  -299.01    -102.74    0.99\n",
      " \n",
      "92 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -594.06    -196.82    2.0\n",
      "Pro  -296.04    -87.12    1.0\n",
      "Ant  -299.01    -109.7    1.0\n",
      " \n",
      "93 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -594.06    -181.48    2.0\n",
      "Pro  -296.04    -80.6    1.0\n",
      "Ant  -299.01    -100.87    1.0\n",
      " \n",
      "94 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -594.06    -163.74    2.0\n",
      "Pro  -296.04    -64.29    1.0\n",
      "Ant  -299.01    -99.45    1.0\n",
      " \n",
      "95 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -594.06    -157.95    2.0\n",
      "Pro  -296.04    -63.13    1.0\n",
      "Ant  -299.01    -94.83    1.0\n",
      " \n",
      "96 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -594.06    -150.61    2.0\n",
      "Pro  -295.05    -62.29    1.0\n",
      "Ant  -299.01    -88.32    1.0\n",
      " \n",
      "97 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -594.06    -161.83    2.0\n",
      "Pro  -297.03    -65.4    1.0\n",
      "Ant  -299.01    -96.43    1.0\n",
      " \n",
      "98 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -594.06    -144.46    2.0\n",
      "Pro  -297.03    -59.0    1.0\n",
      "Ant  -299.01    -85.47    1.0\n",
      " \n",
      "99 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -579.21    -130.85    2.0\n",
      "Pro  -297.03    -54.13    1.0\n",
      "Ant  -299.01    -76.73    1.0\n",
      " \n",
      "100 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -579.21    -132.5    2.0\n",
      "Pro  -297.03    -55.8    1.0\n",
      "Ant  -299.01    -76.7    1.0\n",
      " \n",
      "101 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -579.21    -129.88    2.0\n",
      "Pro  -297.03    -55.28    1.0\n",
      "Ant  -297.03    -74.6    1.0\n",
      " \n",
      "102 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -579.21    -118.03    2.0\n",
      "Pro  -297.03    -53.24    1.0\n",
      "Ant  -297.03    -64.79    1.0\n",
      " \n",
      "103 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -594.06    -123.45    2.0\n",
      "Pro  -297.03    -55.61    1.0\n",
      "Ant  -298.02    -67.85    1.0\n",
      " \n",
      "104 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -130.68    1.96\n",
      "Pro  -299.01    -65.27    0.99\n",
      "Ant  -299.01    -65.41    1.0\n",
      " \n",
      "105 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -136.67    1.98\n",
      "Pro  -299.01    -70.67    0.99\n",
      "Ant  -299.01    -65.99    1.0\n",
      " \n",
      "106 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -131.58    1.98\n",
      "Pro  -299.01    -69.16    0.99\n",
      "Ant  -299.01    -62.42    0.99\n",
      " \n",
      "107 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -138.75    1.98\n",
      "Pro  -299.01    -69.06    0.99\n",
      "Ant  -299.01    -69.69    0.99\n",
      " \n",
      "108 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -148.72    1.98\n",
      "Pro  -299.01    -80.57    1.0\n",
      "Ant  -299.01    -68.14    0.99\n",
      " \n",
      "109 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -156.55    1.98\n",
      "Pro  -299.01    -88.71    1.0\n",
      "Ant  -299.01    -67.85    0.99\n",
      " \n",
      "110 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -172.76    1.98\n",
      "Pro  -299.01    -97.12    1.0\n",
      "Ant  -299.01    -75.64    0.99\n",
      " \n",
      "111 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -598.02    -157.78    1.98\n",
      "Pro  -299.01    -86.5    1.0\n",
      "Ant  -299.01    -71.28    0.99\n",
      " \n",
      "112 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -592.08    -153.0    1.92\n",
      "Pro  -299.01    -84.13    1.0\n",
      "Ant  -296.04    -68.86    0.99\n",
      " \n",
      "113 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -592.08    -157.19    1.92\n",
      "Pro  -299.01    -90.12    1.0\n",
      "Ant  -296.04    -67.06    0.99\n",
      " \n",
      "114 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -592.08    -162.89    1.92\n",
      "Pro  -299.01    -89.76    1.0\n",
      "Ant  -298.02    -73.13    1.0\n",
      " \n",
      "115 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -584.16    -152.09    1.92\n",
      "Pro  -299.01    -84.64    1.0\n",
      "Ant  -298.02    -67.45    1.0\n",
      " \n",
      "116 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -584.16    -152.39    1.98\n",
      "Pro  -299.01    -79.41    1.0\n",
      "Ant  -298.02    -72.98    1.0\n",
      " \n",
      "117 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -564.36    -138.36    1.98\n",
      "Pro  -299.01    -70.27    1.0\n",
      "Ant  -298.02    -68.1    1.0\n",
      " \n",
      "118 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -564.36    -127.84    1.98\n",
      "Pro  -299.01    -61.66    0.99\n",
      "Ant  -298.02    -66.18    1.0\n",
      " \n",
      "119 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -541.59    -107.93    2.0\n",
      "Pro  -293.07    -51.46    1.0\n",
      "Ant  -298.02    -56.47    1.0\n",
      " \n",
      "120 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -541.59    -104.07    2.0\n",
      "Pro  -293.07    -45.8    1.0\n",
      "Ant  -298.02    -58.28    1.0\n",
      " \n",
      "121 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -541.59    -87.88    2.0\n",
      "Pro  -293.07    -37.63    1.0\n",
      "Ant  -298.02    -50.25    1.0\n",
      " \n",
      "122 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -541.59    -83.87    2.0\n",
      "Pro  -253.47    -26.63    1.0\n",
      "Ant  -298.02    -57.24    1.0\n",
      " \n",
      "123 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -495.06    -76.89    2.0\n",
      "Pro  -264.36    -26.18    1.0\n",
      "Ant  -298.02    -50.71    1.0\n",
      " \n",
      "124 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -495.06    -68.36    2.0\n",
      "Pro  -264.36    -22.72    1.0\n",
      "Ant  -298.02    -45.64    1.0\n",
      " \n",
      "125 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -495.06    -67.96    2.0\n",
      "Pro  -281.19    -23.59    1.0\n",
      "Ant  -298.02    -44.37    1.0\n",
      " \n",
      "126 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -495.06    -77.29    2.0\n",
      "Pro  -281.19    -24.71    1.0\n",
      "Ant  -298.02    -52.58    1.0\n",
      " \n",
      "127 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -495.06    -77.97    0.54\n",
      "Pro  -296.04    -26.4    1.0\n",
      "Ant  -298.02    -51.58    0.99\n",
      " \n",
      "128 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -376.26    -73.65    1.97\n",
      "Pro  -296.04    -27.71    1.0\n",
      "Ant  -297.03    -45.94    0.99\n",
      " \n",
      "129 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -376.26    -68.36    1.97\n",
      "Pro  -296.04    -29.84    1.0\n",
      "Ant  -297.03    -38.52    0.99\n",
      " \n",
      "130 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -376.26    -64.72    1.97\n",
      "Pro  -296.04    -26.36    1.0\n",
      "Ant  -297.03    -38.37    0.99\n",
      " \n",
      "131 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -376.26    -60.91    1.97\n",
      "Pro  -296.04    -26.45    1.0\n",
      "Ant  -297.03    -34.46    0.99\n",
      " \n",
      "132 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -566.34    -68.76    1.97\n",
      "Pro  -296.04    -34.33    1.0\n",
      "Ant  -297.03    -34.43    0.99\n",
      " \n",
      "133 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -566.34    -58.07    1.97\n",
      "Pro  -296.04    -27.92    1.0\n",
      "Ant  -297.03    -30.15    1.0\n",
      " \n",
      "134 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -566.34    -55.67    1.97\n",
      "Pro  -296.04    -30.52    1.0\n",
      "Ant  -294.06    -25.15    1.0\n",
      " \n",
      "135 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -566.34    -53.91    1.97\n",
      "Pro  -291.09    -27.42    1.0\n",
      "Ant  -294.06    -26.49    1.0\n",
      " \n",
      "136 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -566.34    -49.79    1.98\n",
      "Pro  -291.09    -22.82    1.0\n",
      "Ant  -294.06    -26.97    1.0\n",
      " \n",
      "137 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -566.34    -49.69    1.98\n",
      "Pro  -291.09    -24.03    1.0\n",
      "Ant  -294.06    -25.66    1.0\n",
      " \n",
      "138 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -566.34    -53.19    1.98\n",
      "Pro  -291.09    -28.33    1.0\n",
      "Ant  -294.06    -24.86    1.0\n",
      " \n",
      "139 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -566.34    -52.76    1.98\n",
      "Pro  -290.1    -26.77    1.0\n",
      "Ant  -294.06    -25.99    1.0\n",
      " \n",
      "140 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -433.68    -42.56    1.98\n",
      "Pro  -290.1    -20.52    1.0\n",
      "Ant  -271.29    -22.04    1.0\n",
      " \n",
      "141 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -433.68    -41.76    1.98\n",
      "Pro  -290.1    -19.7    1.0\n",
      "Ant  -271.29    -22.05    1.0\n",
      " \n",
      "142 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -433.68    -37.42    1.98\n",
      "Pro  -289.11    -17.35    1.0\n",
      "Ant  -248.52    -20.07    1.0\n",
      " \n",
      "143 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -433.68    -34.9    1.98\n",
      "Pro  -289.11    -14.27    1.0\n",
      "Ant  -248.52    -20.64    1.0\n",
      " \n",
      "144 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -433.68    -29.34    1.98\n",
      "Pro  -289.11    -11.56    1.0\n",
      "Ant  -248.52    -17.78    1.0\n",
      " \n",
      "145 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -237.66    -20.71    1.98\n",
      "Pro  -103.0    -6.79    1.0\n",
      "Ant  -227.73    -13.91    1.0\n",
      " \n",
      "146 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -237.66    -16.39    1.98\n",
      "Pro  -66.36    -4.57    1.0\n",
      "Ant  -227.73    -11.82    1.0\n",
      " \n",
      "147 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -237.66    -16.73    1.99\n",
      "Pro  -87.15    -4.6    1.0\n",
      "Ant  -227.73    -12.13    1.0\n",
      " \n",
      "148 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -237.66    -17.42    2.0\n",
      "Pro  -119.82    -5.59    1.0\n",
      "Ant  -227.73    -11.82    1.0\n",
      " \n",
      "149 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -172.28    -12.81    2.0\n",
      "Pro  -119.82    -4.57    1.0\n",
      "Ant  -173.28    -8.24    1.0\n",
      " \n",
      "150 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -146.58    -9.92    2.0\n",
      "Pro  -119.82    -3.63    1.0\n",
      "Ant  -141.6    -6.3    1.0\n",
      " \n",
      "151 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -211.88    -13.48    2.0\n",
      "Pro  -156.45    -5.82    1.0\n",
      "Ant  -212.88    -7.66    1.0\n",
      " \n",
      "152 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -211.88    -13.5    2.0\n",
      "Pro  -156.45    -6.31    1.0\n",
      "Ant  -212.88    -7.19    1.0\n",
      " \n",
      "153 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -211.88    -12.73    2.0\n",
      "Pro  -156.45    -5.06    1.0\n",
      "Ant  -212.88    -7.66    1.0\n",
      " \n",
      "154 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -211.88    -9.42    2.0\n",
      "Pro  -156.45    -3.62    1.0\n",
      "Ant  -212.88    -5.81    1.0\n",
      " \n",
      "155 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -211.88    -9.53    2.0\n",
      "Pro  -156.45    -3.72    1.0\n",
      "Ant  -212.88    -5.81    1.0\n",
      " \n",
      "156 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -143.57    -4.25    2.0\n",
      "Pro  -65.37    -1.04    1.0\n",
      "Ant  -144.57    -3.21    1.0\n",
      " \n",
      "157 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -47.67    -1.35    2.0\n",
      "Pro  -48.54    -0.24    1.0\n",
      "Ant  -46.56    -1.11    1.0\n",
      " \n",
      "158 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -70.35    -1.14    2.0\n",
      "Pro  -59.43    -0.61    1.0\n",
      "Ant  -10.92    -0.53    1.0\n",
      " \n",
      "159 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -70.35    -0.6    2.0\n",
      "Pro  -59.43    -0.53    1.0\n",
      "Ant  -10.92    -0.07    1.0\n",
      " \n",
      "160 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -4.98    1.32    2.0\n",
      "Pro  -5.97    0.84    1.0\n",
      "Ant  -5.97    0.48    1.0\n",
      " \n",
      "161 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -6.11    1.17    2.0\n",
      "Pro  -6.96    0.75    1.0\n",
      "Ant  -4.98    0.43    1.0\n",
      " \n",
      "162 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -295.04    -4.8    2.0\n",
      "Pro  -285.15    -2.1    1.0\n",
      "Ant  -296.04    -2.7    1.0\n",
      " \n",
      "163 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -295.04    -4.66    1.99\n",
      "Pro  -285.15    -2.04    1.0\n",
      "Ant  -296.04    -2.61    0.99\n",
      " \n",
      "164 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -52.49    0.51    1.99\n",
      "Pro  -4.98    0.65    1.0\n",
      "Ant  -53.49    -0.14    0.99\n",
      " \n",
      "165 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -53.35    0.06    1.99\n",
      "Pro  -6.96    0.66    1.0\n",
      "Ant  -54.35    -0.6    0.99\n",
      " \n",
      "166 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -123.77    0.12    1.99\n",
      "Pro  -4.98    0.85    1.0\n",
      "Ant  -124.77    -0.73    1.0\n",
      " \n",
      "167 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -123.77    0.2    2.0\n",
      "Pro  -7.95    0.72    1.0\n",
      "Ant  -124.77    -0.52    1.0\n",
      " \n",
      "168 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.99    1.7    2.0\n",
      "Pro  -3.0    0.93    1.0\n",
      "Ant  -3.99    0.77    1.0\n",
      " \n",
      "169 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.99    1.8    2.0\n",
      "Pro  -3.0    0.97    1.0\n",
      "Ant  -3.99    0.83    1.0\n",
      " \n",
      "170 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -292.46    -1.03    2.0\n",
      "Pro  -293.07    -1.62    1.0\n",
      "Ant  -15.25    0.59    1.0\n",
      " \n",
      "171 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -13.88    1.65    1.99\n",
      "Pro  -3.0    0.94    1.0\n",
      "Ant  -14.88    0.71    1.0\n",
      " \n",
      "172 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.99    1.82    2.0\n",
      "Pro  -3.0    0.94    1.0\n",
      "Ant  -3.99    0.88    1.0\n",
      " \n",
      "173 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -109.91    1.13    2.0\n",
      "Pro  -3.99    0.94    1.0\n",
      "Ant  -110.91    0.19    1.0\n",
      " \n",
      "174 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -44.57    1.52    2.0\n",
      "Pro  -3.0    0.97    1.0\n",
      "Ant  -45.57    0.55    1.0\n",
      " \n",
      "175 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -16.85    1.57    2.0\n",
      "Pro  -3.0    0.93    1.0\n",
      "Ant  -17.85    0.63    1.0\n",
      " \n",
      "176 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -57.44    1.14    2.0\n",
      "Pro  -4.98    0.84    1.0\n",
      "Ant  -58.44    0.3    1.0\n",
      " \n",
      "177 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -34.67    1.54    2.0\n",
      "Pro  -3.0    0.93    1.0\n",
      "Ant  -35.67    0.61    1.0\n",
      " \n",
      "178 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -10.91    1.81    2.0\n",
      "Pro  -3.0    0.95    1.0\n",
      "Ant  -11.91    0.86    1.0\n",
      " \n",
      "179 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -180.2    -0.27    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -181.2    -1.27    1.0\n",
      " \n",
      "180 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -93.08    1.32    2.0\n",
      "Pro  -3.0    0.98    1.0\n",
      "Ant  -94.08    0.34    1.0\n",
      " \n",
      "181 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.01    1.94    2.0\n",
      "Pro  -3.0    0.96    1.0\n",
      "Ant  -0.04    0.98    1.0\n",
      " \n",
      "182 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -129.71    1.25    2.0\n",
      "Pro  -3.0    0.98    1.0\n",
      "Ant  -130.71    0.27    1.0\n",
      " \n",
      "183 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -79.22    1.28    2.0\n",
      "Pro  -3.0    0.98    1.0\n",
      "Ant  -80.22    0.3    1.0\n",
      " \n",
      "184 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.05    1.96    2.0\n",
      "Pro  -3.0    0.98    1.0\n",
      "Ant  0.89    0.98    1.0\n",
      " \n",
      "185 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -112.88    1.27    2.0\n",
      "Pro  -4.98    0.97    1.0\n",
      "Ant  -113.88    0.31    1.0\n",
      " \n",
      "186 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -9.99    1.92    2.0\n",
      "Pro  -10.92    0.95    1.0\n",
      "Ant  -2.01    0.97    1.0\n",
      " \n",
      "187 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -265.39    -9.67    2.0\n",
      "Pro  -266.34    -5.03    1.0\n",
      "Ant  -194.07    -4.64    1.0\n",
      " \n",
      "188 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -293.06    -7.51    2.0\n",
      "Pro  -181.2    -1.7    1.0\n",
      "Ant  -294.06    -5.81    1.0\n",
      " \n",
      "189 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -90.11    -0.34    2.0\n",
      "Pro  -47.55    0.27    1.0\n",
      "Ant  -91.11    -0.61    1.0\n",
      " \n",
      "190 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -106.94    0.6    2.0\n",
      "Pro  -104.97    0.41    1.0\n",
      "Ant  -107.94    0.19    1.0\n",
      " \n",
      "191 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -119.84    0.6    2.0\n",
      "Pro  -120.81    -0.02    1.0\n",
      "Ant  -53.49    0.62    1.0\n",
      " \n",
      "192 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -141.59    0.54    2.0\n",
      "Pro  -142.59    0.2    1.0\n",
      "Ant  -84.18    0.34    1.0\n",
      " \n",
      "193 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -39.62    1.38    2.0\n",
      "Pro  -4.03    0.93    1.0\n",
      "Ant  -40.62    0.45    1.0\n",
      " \n",
      "194 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -17.93    1.72    2.0\n",
      "Pro  -18.84    0.81    1.0\n",
      "Ant  -8.03    0.91    1.0\n",
      " \n",
      "195 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -70.36    1.61    2.0\n",
      "Pro  -71.31    0.63    1.0\n",
      "Ant  -0.03    0.97    1.0\n",
      " \n",
      "196 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -30.71    1.77    2.0\n",
      "Pro  -3.99    0.97    1.0\n",
      "Ant  -31.71    0.8    1.0\n",
      " \n",
      "197 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -295.04    -0.43    2.0\n",
      "Pro  -55.47    0.6    1.0\n",
      "Ant  -296.04    -1.03    1.0\n",
      " \n",
      "198 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -107.93    1.46    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -108.93    0.46    1.0\n",
      " \n",
      "199 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -47.54    1.61    2.0\n",
      "Pro  -3.99    0.97    1.0\n",
      "Ant  -48.54    0.64    1.0\n",
      " \n",
      "200 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -0.95    1.97    2.0\n",
      "Pro  0.51    1.0    1.0\n",
      "Ant  -1.95    0.97    1.0\n",
      " \n",
      "201 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -224.75    1.08    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -225.75    0.08    1.0\n",
      " \n",
      "202 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.99    1.96    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -3.99    0.96    1.0\n",
      " \n",
      "203 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -36.65    1.76    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -37.65    0.76    1.0\n",
      " \n",
      "204 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -297.02    -15.38    2.0\n",
      "Pro  -3.0    0.94    1.0\n",
      "Ant  -298.02    -16.32    1.0\n",
      " \n",
      "205 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -292.07    -20.38    2.0\n",
      "Pro  -3.0    0.94    1.0\n",
      "Ant  -293.07    -21.32    1.0\n",
      " \n",
      "206 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -289.1    -14.17    2.0\n",
      "Pro  -3.99    0.93    1.0\n",
      "Ant  -290.1    -15.1    1.0\n",
      " \n",
      "207 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -287.12    -5.96    2.0\n",
      "Pro  0.0    0.99    1.0\n",
      "Ant  -288.12    -6.94    1.0\n",
      " \n",
      "208 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -272.27    0.18    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -273.27    -0.82    1.0\n",
      " \n",
      "209 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -16.85    1.78    2.0\n",
      "Pro  -3.99    0.95    1.0\n",
      "Ant  -17.85    0.84    1.0\n",
      " \n",
      "210 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -254.5    -4.97    2.0\n",
      "Pro  -255.45    -0.87    1.0\n",
      "Ant  -252.48    -4.1    1.0\n",
      " \n",
      "211 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -5.96    1.87    2.0\n",
      "Pro  -3.0    0.98    1.0\n",
      "Ant  -6.96    0.9    1.0\n",
      " \n",
      "212 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -29.75    1.5    2.0\n",
      "Pro  -30.72    0.74    1.0\n",
      "Ant  -8.94    0.75    1.0\n",
      " \n",
      "213 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -21.85    1.79    2.0\n",
      "Pro  -22.8    0.88    1.0\n",
      "Ant  -11.91    0.91    1.0\n",
      " \n",
      "214 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -45.07    1.48    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -46.07    0.48    1.0\n",
      " \n",
      "215 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -121.79    1.22    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -122.79    0.22    1.0\n",
      " \n",
      "216 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -3.02    1.91    2.0\n",
      "Pro  -3.99    0.98    1.0\n",
      "Ant  -3.0    0.93    1.0\n",
      " \n",
      "217 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -37.64    1.7    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -38.64    0.7    1.0\n",
      " \n",
      "218 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -289.1    0.08    2.0\n",
      "Pro  -3.0    0.98    1.0\n",
      "Ant  -290.1    -0.9    1.0\n",
      " \n",
      "219 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.99    1.91    2.0\n",
      "Pro  0.85    1.0    1.0\n",
      "Ant  -3.99    0.91    1.0\n",
      " \n",
      "220 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -138.21    1.38    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -139.21    0.38    1.0\n",
      " \n",
      "221 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.81    1.98    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.81    0.98    1.0\n",
      " \n",
      "222 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -201.98    0.72    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -202.98    -0.28    1.0\n",
      " \n",
      "223 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -3.98    1.8    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -4.98    0.8    1.0\n",
      " \n",
      "224 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -188.06    -0.28    2.0\n",
      "Pro  -70.32    0.45    1.0\n",
      "Ant  -189.06    -0.72    1.0\n",
      " \n",
      "225 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -11.9    1.6    2.0\n",
      "Pro  -3.99    0.96    1.0\n",
      "Ant  -12.9    0.65    1.0\n",
      " \n",
      "226 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -10.91    1.87    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -11.91    0.87    1.0\n",
      " \n",
      "227 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -141.59    0.73    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -142.59    -0.27    1.0\n",
      " \n",
      "228 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -284.15    -1.58    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -285.15    -2.58    1.0\n",
      " \n",
      "229 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -5.96    1.8    2.0\n",
      "Pro  -3.99    0.97    1.0\n",
      "Ant  -6.96    0.83    1.0\n",
      " \n",
      "230 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -6.95    1.88    2.0\n",
      "Pro  0.0    0.99    1.0\n",
      "Ant  -7.95    0.88    1.0\n",
      " \n",
      "231 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -94.07    0.92    2.0\n",
      "Pro  0.0    0.99    1.0\n",
      "Ant  -95.07    -0.07    1.0\n",
      " \n",
      "232 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -291.08    -2.1    2.0\n",
      "Pro  -281.19    -1.56    1.0\n",
      "Ant  -292.08    -0.55    1.0\n",
      " \n",
      "233 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -51.5    1.66    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -52.5    0.66    1.0\n",
      " \n",
      "234 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -130.7    1.21    2.0\n",
      "Pro  0.0    0.99    1.0\n",
      "Ant  -131.7    0.22    1.0\n",
      " \n",
      "235 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -69.15    1.6    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -70.15    0.6    1.0\n",
      " \n",
      "236 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -230.69    0.72    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -231.69    -0.28    1.0\n",
      " \n",
      "237 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -226.73    0.82    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -227.73    -0.18    1.0\n",
      " \n",
      "238 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -289.1    0.62    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -290.1    -0.38    1.0\n",
      " \n",
      "239 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.93    1.99    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  0.93    0.99    1.0\n",
      " \n",
      "240 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -69.32    1.7    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -70.32    0.7    1.0\n",
      " \n",
      "241 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -278.21    0.79    2.0\n",
      "Pro  -1.0    0.99    1.0\n",
      "Ant  -279.21    -0.2    1.0\n",
      " \n",
      "242 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -113.87    1.49    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -114.87    0.49    1.0\n",
      " \n",
      "243 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -11.9    1.92    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -12.9    0.92    1.0\n",
      " \n",
      "244 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -242.6    0.5    2.0\n",
      "Pro  -243.57    -0.11    1.0\n",
      "Ant  -75.27    0.61    1.0\n",
      " \n",
      "245 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -28.73    1.85    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -29.73    0.85    1.0\n",
      " \n",
      "246 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -295.04    0.2    2.0\n",
      "Pro  -296.04    -0.49    1.0\n",
      "Ant  -51.51    0.69    1.0\n",
      " \n",
      "247 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.0    1.99    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  0.0    0.99    1.0\n",
      " \n",
      "248 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -293.06    -1.86    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -294.06    -2.86    1.0\n",
      " \n",
      "249 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -293.06    -0.19    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -294.06    -1.18    1.0\n",
      " \n",
      "250 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -293.06    -0.07    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -294.06    -1.07    1.0\n",
      " \n",
      "251 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -290.1    -1.9    2.0\n",
      "Pro  -291.09    -0.68    1.0\n",
      "Ant  -285.15    -1.22    1.0\n",
      " \n",
      "252 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -169.31    0.86    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -170.31    -0.14    1.0\n",
      " \n",
      "253 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -75.26    1.24    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -76.26    0.24    1.0\n",
      " \n",
      "254 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -207.03    1.1    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -208.03    0.1    1.0\n",
      " \n",
      "255 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -253.46    0.88    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -254.46    -0.12    1.0\n",
      " \n",
      "256 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -40.62    1.58    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -41.61    0.58    1.0\n",
      " \n",
      "257 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -298.01    0.1    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -299.01    -0.9    1.0\n",
      " \n",
      "258 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -179.21    1.13    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -180.21    0.13    1.0\n",
      " \n",
      "259 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.99    1.93    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -3.99    0.93    1.0\n",
      " \n",
      "260 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -264.35    0.86    2.0\n",
      "Pro  -265.35    -0.1    1.0\n",
      "Ant  -0.02    0.96    1.0\n",
      " \n",
      "261 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -1.03    1.97    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -2.03    0.97    1.0\n",
      " \n",
      "262 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -266.33    0.31    2.0\n",
      "Pro  0.0    0.99    1.0\n",
      "Ant  -267.33    -0.68    1.0\n",
      " \n",
      "263 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -3.98    1.92    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -4.98    0.92    1.0\n",
      " \n",
      "264 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -297.02    0.48    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -298.02    -0.52    1.0\n",
      " \n",
      "265 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -261.38    0.44    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -262.38    -0.56    1.0\n",
      " \n",
      "266 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -16.85    1.81    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -17.85    0.81    1.0\n",
      " \n",
      "267 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -295.05    0.11    2.0\n",
      "Pro  -296.04    -0.73    1.0\n",
      "Ant  -5.0    0.84    1.0\n",
      " \n",
      "268 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -286.13    -0.26    2.0\n",
      "Pro  -9.93    0.94    1.0\n",
      "Ant  -287.13    -1.2    1.0\n",
      " \n",
      "269 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -292.07    0.2    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -293.07    -0.8    1.0\n",
      " \n",
      "270 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -266.11    -0.71    2.0\n",
      "Pro  -97.05    0.51    1.0\n",
      "Ant  -267.11    -1.23    1.0\n",
      " \n",
      "271 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -200.0    -1.66    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -201.0    -2.66    1.0\n",
      " \n",
      "272 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -240.59    0.78    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -241.59    -0.22    1.0\n",
      " \n",
      "273 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.0    1.93    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -3.0    0.93    1.0\n",
      " \n",
      "274 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.99    1.93    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -3.99    0.93    1.0\n",
      " \n",
      "275 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -291.08    -0.45    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -292.08    -1.44    1.0\n",
      " \n",
      "276 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -291.08    -0.52    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -292.08    -1.52    1.0\n",
      " \n",
      "277 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -280.19    0.04    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -281.19    -0.96    1.0\n",
      " \n",
      "278 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -263.36    0.86    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -264.36    -0.14    1.0\n",
      " \n",
      "279 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -205.95    -0.62    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -206.94    -1.62    1.0\n",
      " \n",
      "280 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -175.25    0.35    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -176.25    -0.65    1.0\n",
      " \n",
      "281 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -296.03    0.54    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -297.03    -0.46    1.0\n",
      " \n",
      "282 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -192.0    0.71    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -193.0    -0.29    1.0\n",
      " \n",
      "283 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -259.4    0.87    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -260.4    -0.13    1.0\n",
      " \n",
      "284 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.0    1.96    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -3.0    0.96    1.0\n",
      " \n",
      "285 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -297.02    0.64    2.0\n",
      "Pro  -5.97    0.97    1.0\n",
      "Ant  -298.02    -0.33    1.0\n",
      " \n",
      "286 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -123.77    1.41    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -124.77    0.41    1.0\n",
      " \n",
      "287 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -296.03    -0.62    2.0\n",
      "Pro  -289.11    -0.28    1.0\n",
      "Ant  -297.03    -0.34    1.0\n",
      " \n",
      "288 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -43.59    1.64    2.0\n",
      "Pro  -44.58    0.8    1.0\n",
      "Ant  -31.71    0.84    1.0\n",
      " \n",
      "289 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -32.05    1.8    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -33.05    0.8    1.0\n",
      " \n",
      "290 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.95    1.95    2.0\n",
      "Pro  -0.01    1.0    1.0\n",
      "Ant  -0.05    0.96    1.0\n",
      " \n",
      "291 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -295.04    -0.57    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -296.04    -1.57    1.0\n",
      " \n",
      "292 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -275.24    -0.48    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -276.24    -1.48    1.0\n",
      " \n",
      "293 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.0    1.96    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -1.0    0.96    1.0\n",
      " \n",
      "294 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -293.09    0.53    2.0\n",
      "Pro  -294.06    -0.44    1.0\n",
      "Ant  -0.05    0.98    1.0\n",
      " \n",
      "295 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -289.13    0.62    2.0\n",
      "Pro  -290.1    -0.35    1.0\n",
      "Ant  -3.0    0.96    1.0\n",
      " \n",
      "296 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -211.88    0.07    2.0\n",
      "Pro  -38.64    0.78    1.0\n",
      "Ant  -212.88    -0.71    1.0\n",
      " \n",
      "297 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -289.1    0.59    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -290.1    -0.4    1.0\n",
      " \n",
      "298 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -291.08    0.78    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -292.08    -0.21    1.0\n",
      " \n",
      "299 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -255.44    0.91    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -256.44    -0.09    1.0\n",
      " \n",
      "300 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -39.62    1.76    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -40.62    0.76    1.0\n",
      " \n",
      "301 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -71.3    1.67    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -72.3    0.67    1.0\n",
      " \n",
      "302 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -16.86    1.9    2.0\n",
      "Pro  -17.85    0.92    1.0\n",
      "Ant  -0.02    0.97    1.0\n",
      " \n",
      "303 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -39.62    1.75    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -40.62    0.75    1.0\n",
      " \n",
      "304 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -290.09    -2.53    2.0\n",
      "Pro  -3.0    0.98    1.0\n",
      "Ant  -291.09    -3.51    1.0\n",
      " \n",
      "305 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -265.34    0.95    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -266.34    -0.05    1.0\n",
      " \n",
      "306 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -291.08    -0.73    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -292.08    -1.73    1.0\n",
      " \n",
      "307 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -228.71    0.17    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -229.71    -0.83    1.0\n",
      " \n",
      "308 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -8.03    1.94    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -9.03    0.94    1.0\n",
      " \n",
      "309 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -295.04    0.45    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -296.04    -0.55    1.0\n",
      " \n",
      "310 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -146.54    1.18    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -147.54    0.18    1.0\n",
      " \n",
      "311 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -253.46    0.85    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -254.46    -0.15    1.0\n",
      " \n",
      "312 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -179.21    1.25    2.0\n",
      "Pro  -1.0    0.99    1.0\n",
      "Ant  -180.21    0.25    1.0\n",
      " \n",
      "313 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -293.06    0.82    2.0\n",
      "Pro  -0.01    1.0    1.0\n",
      "Ant  -294.06    -0.18    1.0\n",
      " \n",
      "314 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -104.96    1.04    2.0\n",
      "Pro  -1.07    0.99    1.0\n",
      "Ant  -105.96    0.05    1.0\n",
      " \n",
      "315 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -291.08    -0.4    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -292.08    -1.4    1.0\n",
      " \n",
      "316 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -293.06    0.83    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -294.06    -0.17    1.0\n",
      " \n",
      "317 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -93.09    1.62    2.0\n",
      "Pro  -94.08    0.63    1.0\n",
      "Ant  -0.02    0.99    1.0\n",
      " \n",
      "318 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -0.03    1.98    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -1.03    0.98    1.0\n",
      " \n",
      "319 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -39.85    1.81    2.0\n",
      "Pro  -1.05    0.99    1.0\n",
      "Ant  -40.85    0.82    1.0\n",
      " \n",
      "320 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.93    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.93    0.99    1.0\n",
      " \n",
      "321 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -17.87    1.85    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -18.84    0.85    1.0\n",
      " \n",
      "322 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -80.21    1.61    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -81.21    0.61    1.0\n",
      " \n",
      "323 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -298.01    0.78    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -299.01    -0.22    1.0\n",
      " \n",
      "324 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.92    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -0.08    0.99    1.0\n",
      " \n",
      "325 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -190.1    0.63    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -191.1    -0.37    1.0\n",
      " \n",
      "326 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -33.68    1.69    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -34.68    0.69    1.0\n",
      " \n",
      "327 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -112.88    0.82    2.0\n",
      "Pro  -49.53    0.76    1.0\n",
      "Ant  -113.88    0.06    1.0\n",
      " \n",
      "328 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -12.89    1.88    2.0\n",
      "Pro  -3.0    0.96    1.0\n",
      "Ant  -13.89    0.92    1.0\n",
      " \n",
      "329 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -219.8    0.39    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -220.8    -0.6    1.0\n",
      " \n",
      "330 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -26.75    1.88    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -27.75    0.88    1.0\n",
      " \n",
      "331 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -267.32    0.95    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -268.32    -0.05    1.0\n",
      " \n",
      "332 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -189.11    1.25    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -190.11    0.25    1.0\n",
      " \n",
      "333 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.0    1.99    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  0.0    0.99    1.0\n",
      " \n",
      "334 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -222.77    0.75    2.0\n",
      "Pro  -0.02    0.99    1.0\n",
      "Ant  -223.77    -0.24    1.0\n",
      " \n",
      "335 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.99    1.97    2.0\n",
      "Pro  -3.99    0.98    1.0\n",
      "Ant  -0.02    0.99    1.0\n",
      " \n",
      "336 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -296.03    -3.99    2.0\n",
      "Pro  -297.03    -3.12    1.0\n",
      "Ant  -202.98    -0.87    1.0\n",
      " \n",
      "337 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -252.47    0.99    2.0\n",
      "Pro  -0.01    1.0    1.0\n",
      "Ant  -253.47    -0.0    1.0\n",
      " \n",
      "338 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -201.98    0.78    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -202.98    -0.22    1.0\n",
      " \n",
      "339 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.98    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -0.02    0.99    1.0\n",
      " \n",
      "340 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.94    2.0    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.97    1.0    1.0\n",
      " \n",
      "341 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.98    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -0.02    0.99    1.0\n",
      " \n",
      "342 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -286.13    0.88    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -287.13    -0.12    1.0\n",
      " \n",
      "343 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.95    2.0    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.95    1.0    1.0\n",
      " \n",
      "344 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.99    1.97    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -3.99    0.97    1.0\n",
      " \n",
      "345 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -23.05    1.89    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -24.05    0.89    1.0\n",
      " \n",
      "346 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -285.14    0.75    2.0\n",
      "Pro  -3.99    0.98    1.0\n",
      "Ant  -286.14    -0.23    1.0\n",
      " \n",
      "347 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -265.34    0.95    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -266.34    -0.05    1.0\n",
      " \n",
      "348 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -53.07    1.75    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -54.07    0.75    1.0\n",
      " \n",
      "349 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -13.54    1.94    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -14.51    0.94    1.0\n",
      " \n",
      "350 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -14.03    1.94    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -15.03    0.94    1.0\n",
      " \n",
      "351 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.95    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.95    0.99    1.0\n",
      " \n",
      "352 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.95    1.99    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  0.95    0.99    1.0\n",
      " \n",
      "353 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.95    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.95    1.0    1.0\n",
      " \n",
      "354 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -289.1    0.86    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -290.1    -0.14    1.0\n",
      " \n",
      "355 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -237.62    1.06    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -238.62    0.06    1.0\n",
      " \n",
      "356 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -296.03    0.69    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -297.03    -0.3    1.0\n",
      " \n",
      "357 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.94    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.97    1.0    1.0\n",
      " \n",
      "358 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -102.03    1.6    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -103.03    0.6    1.0\n",
      " \n",
      "359 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.95    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.95    0.99    1.0\n",
      " \n",
      "360 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -297.03    0.73    2.0\n",
      "Pro  -0.01    1.0    1.0\n",
      "Ant  -298.02    -0.27    1.0\n",
      " \n",
      "361 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -32.69    1.69    2.0\n",
      "Pro  -3.0    0.98    1.0\n",
      "Ant  -33.69    0.71    1.0\n",
      " \n",
      "362 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -3.98    1.97    2.0\n",
      "Pro  -4.98    0.98    1.0\n",
      "Ant  0.97    1.0    1.0\n",
      " \n",
      "363 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -284.15    -0.41    2.0\n",
      "Pro  -285.15    -1.39    1.0\n",
      "Ant  -0.05    0.99    1.0\n",
      " \n",
      "364 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.95    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.95    0.99    1.0\n",
      " \n",
      "365 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.0    1.97    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -1.0    0.97    1.0\n",
      " \n",
      "366 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -297.02    -0.14    2.0\n",
      "Pro  -3.0    0.98    1.0\n",
      "Ant  -298.02    -1.13    1.0\n",
      " \n",
      "367 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.0    1.96    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -1.0    0.96    1.0\n",
      " \n",
      "368 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -67.08    1.71    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -68.08    0.71    1.0\n",
      " \n",
      "369 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -61.82    1.66    2.0\n",
      "Pro  -61.81    0.74    1.0\n",
      "Ant  -9.93    0.92    1.0\n",
      " \n",
      "370 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -296.03    0.63    2.0\n",
      "Pro  -25.77    0.89    1.0\n",
      "Ant  -297.03    -0.26    1.0\n",
      " \n",
      "371 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -294.06    -1.04    2.0\n",
      "Pro  -289.11    -0.53    1.0\n",
      "Ant  -294.06    -0.5    1.0\n",
      " \n",
      "372 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.0    1.97    2.0\n",
      "Pro  -3.0    0.98    1.0\n",
      "Ant  -0.01    0.99    1.0\n",
      " \n",
      "373 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.01    1.95    2.0\n",
      "Pro  -3.0    0.95    1.0\n",
      "Ant  0.97    1.0    1.0\n",
      " \n",
      "374 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -227.72    0.57    2.0\n",
      "Pro  -228.72    -0.43    1.0\n",
      "Ant  0.94    1.0    1.0\n",
      " \n",
      "375 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -288.14    -0.25    2.0\n",
      "Pro  -289.11    -0.82    1.0\n",
      "Ant  -73.29    0.57    1.0\n",
      " \n",
      "376 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -261.38    0.22    2.0\n",
      "Pro  -3.0    0.98    1.0\n",
      "Ant  -262.38    -0.76    1.0\n",
      " \n",
      "377 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -278.21    -0.54    2.0\n",
      "Pro  -279.21    -0.39    1.0\n",
      "Ant  -237.63    -0.15    1.0\n",
      " \n",
      "378 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -293.06    0.54    2.0\n",
      "Pro  -294.06    -0.44    1.0\n",
      "Ant  -1.0    0.98    1.0\n",
      " \n",
      "379 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -294.05    0.54    2.0\n",
      "Pro  -3.0    0.96    1.0\n",
      "Ant  -295.05    -0.41    1.0\n",
      " \n",
      "380 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -577.23    -1.8    2.0\n",
      "Pro  -297.03    -1.54    1.0\n",
      "Ant  -280.2    -0.27    1.0\n",
      " \n",
      "381 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.01    1.98    2.0\n",
      "Pro  -3.0    0.98    1.0\n",
      "Ant  0.97    1.0    1.0\n",
      " \n",
      "382 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.0    1.98    2.0\n",
      "Pro  -3.0    0.98    1.0\n",
      "Ant  0.97    1.0    1.0\n",
      " \n",
      "383 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -296.09    0.81    2.0\n",
      "Pro  -297.03    -0.18    1.0\n",
      "Ant  0.0    0.99    1.0\n",
      " \n",
      "384 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.99    1.97    2.0\n",
      "Pro  -3.99    0.98    1.0\n",
      "Ant  -0.02    0.99    1.0\n",
      " \n",
      "385 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -3.98    1.97    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -4.98    0.97    1.0\n",
      " \n",
      "386 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -280.19    -2.45    2.0\n",
      "Pro  -278.22    -2.1    1.0\n",
      "Ant  -281.19    -0.35    1.0\n",
      " \n",
      "387 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -291.09    0.56    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -292.08    -0.43    1.0\n",
      " \n",
      "388 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -8.04    1.95    2.0\n",
      "Pro  -9.03    0.96    1.0\n",
      "Ant  -0.02    0.99    1.0\n",
      " \n",
      "389 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -294.05    -0.49    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -295.05    -1.49    1.0\n",
      " \n",
      "390 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.97    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -0.03    0.99    1.0\n",
      " \n",
      "391 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -13.88    1.84    2.0\n",
      "Pro  0.71    1.0    1.0\n",
      "Ant  -14.88    0.84    1.0\n",
      " \n",
      "392 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -43.58    1.73    2.0\n",
      "Pro  0.0    0.99    1.0\n",
      "Ant  -44.58    0.74    1.0\n",
      " \n",
      "393 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -6.95    1.92    2.0\n",
      "Pro  -0.01    0.99    1.0\n",
      "Ant  -7.95    0.93    1.0\n",
      " \n",
      "394 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -293.07    0.81    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -294.06    -0.19    1.0\n",
      " \n",
      "395 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -17.64    1.89    2.0\n",
      "Pro  -18.63    0.91    1.0\n",
      "Ant  -3.0    0.98    1.0\n",
      " \n",
      "396 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -44.58    1.8    2.0\n",
      "Pro  -45.57    0.81    1.0\n",
      "Ant  0.0    0.99    1.0\n",
      " \n",
      "397 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.98    1.98    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -0.01    0.99    1.0\n",
      " \n",
      "398 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -297.03    0.58    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -298.02    -0.41    1.0\n",
      " \n",
      "399 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.0    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.0    0.99    1.0\n",
      " \n",
      "400 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -11.06    1.91    2.0\n",
      "Pro  0.0    0.99    1.0\n",
      "Ant  -12.06    0.92    1.0\n",
      " \n",
      "401 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -5.96    1.95    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -6.96    0.95    1.0\n",
      " \n",
      "402 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -229.7    1.02    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -230.7    0.02    1.0\n",
      " \n",
      "403 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -95.07    1.49    2.0\n",
      "Pro  -96.06    0.61    1.0\n",
      "Ant  -26.76    0.88    1.0\n",
      " \n",
      "404 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -290.09    0.72    2.0\n",
      "Pro  -291.09    -0.26    1.0\n",
      "Ant  -3.0    0.98    1.0\n",
      " \n",
      "405 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.98    1.99    2.0\n",
      "Pro  -0.01    1.0    1.0\n",
      "Ant  0.95    0.99    1.0\n",
      " \n",
      "406 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -426.75    -9.62    2.0\n",
      "Pro  -283.17    -4.96    1.0\n",
      "Ant  -297.03    -4.65    1.0\n",
      " \n",
      "407 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -426.75    -23.59    2.0\n",
      "Pro  -299.01    -13.91    1.0\n",
      "Ant  -297.03    -9.69    1.0\n",
      " \n",
      "408 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -311.91    -17.44    2.0\n",
      "Pro  -298.02    -12.81    1.0\n",
      "Ant  -200.01    -4.63    1.0\n",
      " \n",
      "409 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -13.92    1.29    2.0\n",
      "Pro  -8.94    0.85    1.0\n",
      "Ant  -8.94    0.44    1.0\n",
      " \n",
      "410 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -229.7    0.4    2.0\n",
      "Pro  -43.59    0.73    1.0\n",
      "Ant  -230.7    -0.32    1.0\n",
      " \n",
      "411 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -204.95    0.77    2.0\n",
      "Pro  -3.0    0.96    1.0\n",
      "Ant  -205.95    -0.19    1.0\n",
      " \n",
      "412 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -269.33    0.63    2.0\n",
      "Pro  -270.3    -0.31    1.0\n",
      "Ant  -3.99    0.94    1.0\n",
      " \n",
      "413 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -292.1    0.48    2.0\n",
      "Pro  0.0    0.99    1.0\n",
      "Ant  -293.07    -0.51    1.0\n",
      " \n",
      "414 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -5.96    1.92    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -6.96    0.92    1.0\n",
      " \n",
      "415 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -290.09    0.63    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -291.09    -0.37    1.0\n",
      " \n",
      "416 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.0    1.96    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -3.0    0.97    1.0\n",
      " \n",
      "417 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -42.62    1.74    2.0\n",
      "Pro  -43.59    0.78    1.0\n",
      "Ant  -3.0    0.96    1.0\n",
      " \n",
      "418 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -295.04    0.4    2.0\n",
      "Pro  -26.76    0.87    1.0\n",
      "Ant  -296.04    -0.46    1.0\n",
      " \n",
      "419 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.98    1.95    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -0.02    0.96    1.0\n",
      " \n",
      "420 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -232.68    1.02    2.0\n",
      "Pro  -233.67    0.06    1.0\n",
      "Ant  -0.05    0.96    1.0\n",
      " \n",
      "421 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -285.14    0.43    2.0\n",
      "Pro  -62.4    0.72    1.0\n",
      "Ant  -286.14    -0.28    1.0\n",
      " \n",
      "422 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -227.73    1.04    2.0\n",
      "Pro  -228.72    0.06    1.0\n",
      "Ant  -0.02    0.99    1.0\n",
      " \n",
      "423 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -291.08    0.65    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -292.08    -0.35    1.0\n",
      " \n",
      "424 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -226.73    0.95    2.0\n",
      "Pro  -3.99    0.97    1.0\n",
      "Ant  -227.73    -0.02    1.0\n",
      " \n",
      "425 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -31.7    1.85    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -32.7    0.85    1.0\n",
      " \n",
      "426 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.96    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.97    0.99    1.0\n",
      " \n",
      "427 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.98    1.98    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -0.02    0.98    1.0\n",
      " \n",
      "428 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.0    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.0    0.99    1.0\n",
      " \n",
      "429 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -1.03    1.96    2.0\n",
      "Pro  0.87    1.0    1.0\n",
      "Ant  -2.03    0.97    1.0\n",
      " \n",
      "430 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.99    1.97    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -3.99    0.97    1.0\n",
      " \n",
      "431 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.0    1.97    2.0\n",
      "Pro  -0.02    1.0    1.0\n",
      "Ant  -3.0    0.97    1.0\n",
      " \n",
      "432 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.97    1.98    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -0.03    0.98    1.0\n",
      " \n",
      "433 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -0.02    1.98    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -0.02    0.98    1.0\n",
      " \n",
      "434 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -293.09    0.8    2.0\n",
      "Pro  -294.06    -0.18    1.0\n",
      "Ant  -0.03    0.99    1.0\n",
      " \n",
      "435 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.96    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.97    0.99    1.0\n",
      " \n",
      "436 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -199.01    1.19    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -200.01    0.19    1.0\n",
      " \n",
      "437 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -119.81    1.5    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -120.81    0.5    1.0\n",
      " \n",
      "438 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.98    1.99    2.0\n",
      "Pro  0.95    1.0    1.0\n",
      "Ant  -0.02    0.99    1.0\n",
      " \n",
      "439 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.0    1.96    2.0\n",
      "Pro  -0.01    1.0    1.0\n",
      "Ant  -3.0    0.97    1.0\n",
      " \n",
      "440 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.97    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -0.03    0.99    1.0\n",
      " \n",
      "441 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -32.72    1.86    2.0\n",
      "Pro  -33.69    0.86    1.0\n",
      "Ant  0.97    0.99    1.0\n",
      " \n",
      "442 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -119.82    1.39    2.0\n",
      "Pro  -120.81    0.4    1.0\n",
      "Ant  -1.0    0.99    1.0\n",
      " \n",
      "443 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -19.82    1.91    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -20.82    0.91    1.0\n",
      " \n",
      "444 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.94    1.99    2.0\n",
      "Pro  -0.01    1.0    1.0\n",
      "Ant  -0.03    0.99    1.0\n",
      " \n",
      "445 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.96    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -0.04    0.99    1.0\n",
      " \n",
      "446 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -15.05    1.86    2.0\n",
      "Pro  -1.0    0.99    1.0\n",
      "Ant  -16.05    0.87    1.0\n",
      " \n",
      "447 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -2.0    1.97    2.0\n",
      "Pro  -0.02    1.0    1.0\n",
      "Ant  -3.0    0.97    1.0\n",
      " \n",
      "448 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.96    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.97    0.99    1.0\n",
      " \n",
      "449 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -297.02    -0.85    2.0\n",
      "Pro  -3.0    0.98    1.0\n",
      "Ant  -298.02    -1.83    1.0\n",
      " \n",
      "450 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -297.02    0.79    2.0\n",
      "Pro  -0.02    1.0    1.0\n",
      "Ant  -298.02    -0.2    1.0\n",
      " \n",
      "451 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.91    1.99    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  0.91    0.99    1.0\n",
      " \n",
      "452 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -288.11    -1.79    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -289.11    -2.79    1.0\n",
      " \n",
      "453 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -300.03    0.74    2.0\n",
      "Pro  -4.98    0.98    1.0\n",
      "Ant  -295.05    -0.24    1.0\n",
      " \n",
      "454 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -296.03    0.82    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -297.03    -0.18    1.0\n",
      " \n",
      "455 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -289.1    0.84    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -290.1    -0.16    1.0\n",
      " \n",
      "456 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -287.12    0.87    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -288.12    -0.13    1.0\n",
      " \n",
      "457 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -3.02    1.97    2.0\n",
      "Pro  -3.99    0.98    1.0\n",
      "Ant  0.97    0.99    1.0\n",
      " \n",
      "458 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -293.07    0.84    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -294.06    -0.16    1.0\n",
      " \n",
      "459 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.94    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.94    1.0    1.0\n",
      " \n",
      "460 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -260.39    0.95    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -261.39    -0.05    1.0\n",
      " \n",
      "461 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.0    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.0    0.99    1.0\n",
      " \n",
      "462 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -297.03    -2.49    2.0\n",
      "Pro  -284.16    -0.49    1.0\n",
      "Ant  -298.02    -1.99    1.0\n",
      " \n",
      "463 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -177.23    1.29    2.0\n",
      "Pro  0.0    1.0    1.0\n",
      "Ant  -178.23    0.3    1.0\n",
      " \n",
      "464 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -297.03    0.82    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -298.02    -0.18    1.0\n",
      " \n",
      "465 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -229.7    0.99    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -230.7    -0.01    1.0\n",
      " \n",
      "466 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -293.07    0.5    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -294.06    -0.5    1.0\n",
      " \n",
      "467 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.98    1.99    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -0.02    0.99    1.0\n",
      " \n",
      "468 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -289.1    -0.42    2.0\n",
      "Pro  -270.3    -0.16    1.0\n",
      "Ant  -290.1    -0.26    1.0\n",
      " \n",
      "469 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -276.26    0.89    2.0\n",
      "Pro  -277.23    -0.1    1.0\n",
      "Ant  -0.08    0.99    1.0\n",
      " \n",
      "470 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -171.29    0.79    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -172.29    -0.21    1.0\n",
      " \n",
      "471 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -233.66    0.75    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -234.66    -0.25    1.0\n",
      " \n",
      "472 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -100.02    1.56    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -101.01    0.56    1.0\n",
      " \n",
      "473 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -281.18    -1.39    2.0\n",
      "Pro  -270.3    -1.15    1.0\n",
      "Ant  -282.18    -0.24    1.0\n",
      " \n",
      "474 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.97    2.0    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  0.97    1.0    1.0\n",
      " \n",
      "475 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -295.04    0.81    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -296.04    -0.19    1.0\n",
      " \n",
      "476 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -0.08    1.98    2.0\n",
      "Pro  -1.03    0.99    1.0\n",
      "Ant  -0.03    0.99    1.0\n",
      " \n",
      "477 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -129.71    1.5    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -130.71    0.5    1.0\n",
      " \n",
      "478 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -212.87    1.13    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -213.87    0.13    1.0\n",
      " \n",
      "479 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -292.07    0.64    2.0\n",
      "Pro  -40.03    0.83    1.0\n",
      "Ant  -293.07    -0.19    1.0\n",
      " \n",
      "480 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.95    1.99    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  0.95    0.99    1.0\n",
      " \n",
      "481 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -288.12    -0.55    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -289.11    -1.55    1.0\n",
      " \n",
      "482 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -297.02    0.76    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -298.02    -0.24    1.0\n",
      " \n",
      "483 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.93    1.99    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -0.07    0.99    1.0\n",
      " \n",
      "484 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.97    1.99    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  0.97    0.99    1.0\n",
      " \n",
      "485 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -110.9    1.56    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -111.9    0.56    1.0\n",
      " \n",
      "486 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -22.05    1.9    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -23.05    0.9    1.0\n",
      " \n",
      "487 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.0    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.0    0.99    1.0\n",
      " \n",
      "488 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -297.02    -0.4    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -298.02    -1.4    1.0\n",
      " \n",
      "489 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.98    1.99    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  -0.02    0.99    1.0\n",
      " \n",
      "490 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.98    2.0    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  0.99    1.0    1.0\n",
      " \n",
      "491 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.99    1.99    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -0.01    0.99    1.0\n",
      " \n",
      "492 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.96    2.0    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  0.97    1.0    1.0\n",
      " \n",
      "493 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.99    2.0    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  0.99    1.0    1.0\n",
      " \n",
      "494 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  0.99    1.99    2.0\n",
      "Pro  1.0    1.0    1.0\n",
      "Ant  -0.01    0.99    1.0\n",
      " \n",
      "495 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.0    1.99    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  0.0    0.99    1.0\n",
      " \n",
      "496 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -295.04    0.83    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -296.04    -0.17    1.0\n",
      " \n",
      "497 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.0    1.99    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  0.0    0.99    1.0\n",
      " \n",
      "498 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.95    2.0    2.0\n",
      "Pro  0.97    1.0    1.0\n",
      "Ant  0.95    1.0    1.0\n",
      " \n",
      "499 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  1.96    2.0    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  0.97    1.0    1.0\n",
      " \n",
      "500 Agent -  min - mean - max \n",
      "-----------------------------------------------------\n",
      "Adv  -224.75    1.1    2.0\n",
      "Pro  0.99    1.0    1.0\n",
      "Ant  -225.75    0.1    1.0\n",
      " \n"
     ]
    }
   ],
   "source": [
    "status = \"{:2d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:4.2f} saved {}\"\n",
    "n_iter = 500\n",
    "for n in range(n_iter):\n",
    "    result = agent.train()\n",
    "    chkpt_file = agent.save(chkpt_root)\n",
    "    min_reward = result['policy_reward_min']\n",
    "    mean_reward  = result['policy_reward_mean']\n",
    "    max_reward = result['policy_reward_max']\n",
    "    id0 = \"ppo_policy_0\"\n",
    "    id1 = \"ppo_policy_1\"\n",
    "    id2 = \"ppo_policy_2\"\n",
    "    print(n + 1, 'Agent -  min - mean - max ')\n",
    "    print('-----------------------------------------------------')\n",
    "    print('Adv ', round(min_reward[id0], 2), '  ', round(mean_reward[id0], 2), '  ', round(max_reward[id0], 2))\n",
    "    print('Pro ', round(min_reward[id1], 2), '  ', round(mean_reward[id1], 2), '  ', round(max_reward[id1], 2))\n",
    "    print('Ant ', round(min_reward[id2], 2), '  ', round(mean_reward[id2], 2), '  ', round(max_reward[id2], 2))\n",
    "    print(' ')\n",
    "    \n",
    "    df = df.append({'num': n+1,'adv_min': min_reward[id0],'adv_mean': mean_reward[id0],'adv_max': max_reward[id0], \\\n",
    "                            'pro_min': min_reward[id1],'pro_mean': mean_reward[id1],'pro_max': max_reward[id1], \\\n",
    "                            'ant_min': min_reward[id2],'ant_mean': mean_reward[id2],'ant_max': max_reward[id2]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79dc32eb-c245-41ed-9ba2-940c96a4bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"my_pickle.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24edc1f-877e-4c50-ae82-d73c30cf791b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
