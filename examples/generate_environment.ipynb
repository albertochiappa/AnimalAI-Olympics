{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prerequisite-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "from animalai.envs.arena_config import Vector3, RGB, Item, Arena, ArenaConfig\n",
    "from animalai.envs.environment import AnimalAIEnvironment\n",
    "from mlagents_envs.exception import UnityCommunicationException\n",
    "\n",
    "from typing import List\n",
    "from animalai.communicator_objects import (\n",
    "    ArenasConfigurationsProto,\n",
    "    ArenaConfigurationProto,\n",
    "    ItemToSpawnProto,\n",
    "    VectorProto,\n",
    ")\n",
    "\n",
    "from typing import List\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "naked-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonpickle\n",
    "def save_config(arenas, json_path: str) -> None:\n",
    "    out = jsonpickle.encode(arenas)\n",
    "    out = json.loads(out)\n",
    "    json.dump(out, open(json_path, \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-fleet",
   "metadata": {},
   "source": [
    "## 1. Learning how to generate our own environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "figured-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We redefine the class ArenaConfig to MyArenaConfig so instead of reading a .yaml file.\n",
    "## it reads the arena object we create\n",
    "\n",
    "class ArenaConfig(yaml.YAMLObject):\n",
    "    yaml_tag = u\"!ArenaConfig\"\n",
    "\n",
    "    def __init__(self, my_arena: Arena = None):\n",
    "\n",
    "        self.arenas = {-1: my_arena}\n",
    "\n",
    "    def to_proto(self, seed: int = -1) -> ArenasConfigurationsProto:\n",
    "        arenas_configurations_proto = ArenasConfigurationsProto()\n",
    "        arenas_configurations_proto.seed = seed\n",
    "\n",
    "        for k in self.arenas:\n",
    "            arenas_configurations_proto.arenas[k].CopyFrom(self.arenas[k].to_proto())\n",
    "\n",
    "        return arenas_configurations_proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "proud-savage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!ArenaConfig\n",
      "arenas:\n",
      "  -1: !Arena\n",
      "    pass_mark: 0\n",
      "    t: 250\n",
      "    items:\n",
      "    - !Item\n",
      "      name: Agent\n",
      "      positions:\n",
      "      - !Vector3 {x: 20, y: 0, z: 20}\n",
      "      rotations: [0]\n",
      "    - !Item\n",
      "      name: GoodGoal\n",
      "      positions:\n",
      "      - !Vector3 {x: 20, y: 0, z: 22}\n",
      "      sizes:\n",
      "      - !Vector3 {x: 1, y: 1, z: 1}\n"
     ]
    }
   ],
   "source": [
    "with open('configurations/curriculum/0.yml') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recreate the same curriculum as above\n",
    "\n",
    "# First we create the agent\n",
    "position_agent = Vector3(x = 20, y = 0, z = 20)\n",
    "rotation_agent = 0\n",
    "agent = Item(name = 'Agent', positions = [position_agent], rotations = [rotation_agent])\n",
    "\n",
    "# Then we create the goal\n",
    "position_goal = Vector3(x = 20, y = 0, z = 35)\n",
    "sizes_goal = Vector3(x = 1, y = 1, z = 1)\n",
    "goal = Item(name = 'GoodGoal', positions = [position_goal], sizes = [sizes_goal])\n",
    "\n",
    "# Define list of items\n",
    "items = [agent, goal]\n",
    "\n",
    "# Create Arena\n",
    "my_arena = Arena(t=250, items=items, pass_mark = 0, blackouts = None)\n",
    "\n",
    "# create arena configuration\n",
    "my_config = ArenaConfig(my_arena)\n",
    "my_config.to_proto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize evironment just created\n",
    "\n",
    "try:\n",
    "    environment = AnimalAIEnvironment(\n",
    "            file_name='env/AnimalAI',\n",
    "            base_port=5007,\n",
    "            arenas_configurations=my_config,\n",
    "            play=True,\n",
    "        )\n",
    "except UnityCommunicationException:\n",
    "    # you'll end up here if you close the environment window directly\n",
    "    # always try to close it from script\n",
    "    environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "if environment:\n",
    "    environment.close() # takes a few seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-glasgow",
   "metadata": {},
   "source": [
    "## 2. Function to create environment config given position of goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "burning-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For simplicity, we only modify the position of the goal (position of agent is fixed)\n",
    "\n",
    "def create_arena(x, y):\n",
    "    '''\n",
    "    Create an arena with only one agent and one goal.\n",
    "    :param x: float. x position goal\n",
    "    :param x: float. y position goal\n",
    "    '''\n",
    "    \n",
    "    # Create agent\n",
    "    position_agent = Vector3(x = 20, y = 0, z = 20)\n",
    "    rotation_agent = 0\n",
    "    agent = Item(name = 'Agent', positions = [position_agent], rotations = [rotation_agent])\n",
    "\n",
    "    # Create the goal\n",
    "    position_goal = Vector3(x = x, y = 0, z = y)\n",
    "    sizes_goal = Vector3(x = 1, y = 1, z = 1)\n",
    "    goal = Item(name = 'GoodGoal', positions = [position_goal], sizes = [sizes_goal])\n",
    "\n",
    "    # Define list of items\n",
    "    items = [agent, goal]\n",
    "\n",
    "    # Create Arena\n",
    "    my_arena = Arena(t=250, items=items, pass_mark = 0, blackouts = None)\n",
    "\n",
    "    # create arena configuration\n",
    "    my_config = ArenaConfig(my_arena)\n",
    "    \n",
    "    return my_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "immune-citizenship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ArenaConfig at 0x7f586b731750>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_arena(x=4, y=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize evironment just created\n",
    "try:\n",
    "    environment = AnimalAIEnvironment(\n",
    "            file_name='env/AnimalAI',\n",
    "            base_port=5007,\n",
    "            arenas_configurations=create_arena(x = 30, y = 5),\n",
    "            play=True,\n",
    "        )\n",
    "except UnityCommunicationException:\n",
    "    # you'll end up here if you close the environment window directly\n",
    "    # always try to close it from script\n",
    "    environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if environment:\n",
    "    environment.close() # takes a few seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-yorkshire",
   "metadata": {},
   "source": [
    "## 3. Train agents in environments created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "suspected-pulse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from mlagents.trainers.trainer_util import load_config;\n",
    "from animalai_train.run_options_aai import RunOptionsAAI;\n",
    "from animalai_train.run_training_aai import run_training_aai;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-candle",
   "metadata": {},
   "source": [
    "# The loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ongoing-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define configurations\n",
    "trainer_config_path = (\n",
    "    \"configurations/training_configurations/train_ml_agents_config_ppo_10fs.yaml\"\n",
    ")\n",
    "environment_path = \"env/AnimalAI\"\n",
    "run_id_protagonist = \"protagonist\"\n",
    "base_port_protagonist = 5005\n",
    "run_id_antagonist = \"antatagonist\"\n",
    "base_port_antagonist = 5006\n",
    "\n",
    "# first we create an easy arena\n",
    "arena_config = create_arena(x=20, y=22)\n",
    "\n",
    "# train protagonist\n",
    "args = RunOptionsAAI(\n",
    "    trainer_config=load_config(trainer_config_path),\n",
    "    env_path=environment_path,\n",
    "    run_id=run_id_protagonist,\n",
    "    base_port=base_port_protagonist,\n",
    "    #load_model=False,\n",
    "    train_model=True,\n",
    "    arena_config=arena_config \n",
    ")\n",
    "logs_dir = \"summaries/\"\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {logs_dir}\n",
    "run_training_aai(0, args)\n",
    "\n",
    "# train antagonist\n",
    "args = RunOptionsAAI(\n",
    "    trainer_config=load_config(trainer_config_path),\n",
    "    env_path=environment_path,\n",
    "    run_id=run_id_antagonist,\n",
    "    base_port=base_port_antagonist,\n",
    "    #load_model=False,\n",
    "    train_model=True,\n",
    "    arena_config=arena_config \n",
    ")\n",
    "logs_dir = \"summaries/\"\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {logs_dir}\n",
    "run_training_aai(1, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "prostate-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_trainer(trainer_config_path, steps_add=1.0e5):\n",
    "    with open(trainer_config_path) as f:\n",
    "         list_doc = yaml.load(f)\n",
    "\n",
    "    #list_doc['AnimalAI']['max_steps']  = 1.0e5\n",
    "    list_doc['AnimalAI']['max_steps'] = float(list_doc['AnimalAI']['max_steps']) + steps_add\n",
    "\n",
    "    with open(trainer_config_path, \"w\") as f:\n",
    "        yaml.dump(list_doc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bound-roulette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 4, y: 17\n",
      "Converting ./models/protagonist/AnimalAI/frozen_graph_def.pb to ./models/protagonist/AnimalAI.nn\n",
      "GLOBALS: 'is_continuous_control', 'version_number', 'memory_size', 'action_output_shape'\n",
      "IN: 'visual_observation_0': [-1, 84, 84, 3] => 'policy/main_graph_0_encoder0/conv_1/BiasAdd'\n",
      "IN: 'vector_observation': [-1, 1, 1, 3] => 'policy/main_graph_0/hidden_0/BiasAdd'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice_1'\n",
      "OUT: 'policy/concat/concat', 'action'\n",
      "DONE: wrote ./models/protagonist/AnimalAI.nn file.\n",
      "PROTAGONIST: \n",
      "Steps:  310000  Cumulative reward:  -0.9651249320362696  Episode Length:  245.775\n",
      "Steps:  410000  Cumulative reward:  -0.5592836298524065  Episode Length:  207.53191489361703\n",
      "Steps:  500000  Cumulative reward:  -0.4974761332375739  Episode Length:  199.42857142857144\n",
      "Converting ./models/antatagonist/AnimalAI/frozen_graph_def.pb to ./models/antatagonist/AnimalAI.nn\n",
      "GLOBALS: 'is_continuous_control', 'version_number', 'memory_size', 'action_output_shape'\n",
      "IN: 'visual_observation_0': [-1, 84, 84, 3] => 'policy/main_graph_0_encoder0/conv_1/BiasAdd'\n",
      "IN: 'vector_observation': [-1, 1, 1, 3] => 'policy/main_graph_0/hidden_0/BiasAdd'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice_1'\n",
      "OUT: 'policy/concat/concat', 'action'\n",
      "DONE: wrote ./models/antatagonist/AnimalAI.nn file.\n",
      "ANTAGONIST: \n",
      "Steps:  310000  Cumulative reward:  -0.6012481655331051  Episode Length:  212.48936170212767\n",
      "Steps:  410000  Cumulative reward:  0.8662258772473745  Episode Length:  31.70684039087948\n",
      "Steps:  500000  Cumulative reward:  0.8576011642308559  Episode Length:  32.96271186440678\n",
      "x: 12, y: 11\n",
      "Converting ./models/protagonist/AnimalAI/frozen_graph_def.pb to ./models/protagonist/AnimalAI.nn\n",
      "GLOBALS: 'is_continuous_control', 'version_number', 'memory_size', 'action_output_shape'\n",
      "IN: 'visual_observation_0': [-1, 84, 84, 3] => 'policy/main_graph_0_encoder0/conv_1/BiasAdd'\n",
      "IN: 'vector_observation': [-1, 1, 1, 3] => 'policy/main_graph_0/hidden_0/BiasAdd'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice_1'\n",
      "OUT: 'policy/concat/concat', 'action'\n",
      "DONE: wrote ./models/protagonist/AnimalAI.nn file.\n",
      "PROTAGONIST: \n",
      "Steps:  510000  Cumulative reward:  0.0576068764785304  Episode Length:  125.34615384615384\n",
      "Steps:  560000  Cumulative reward:  0.4627995052342157  Episode Length:  76.6015625\n",
      "Steps:  600000  Cumulative reward:  0.4687121480026028  Episode Length:  74.91666666666667\n",
      "Converting ./models/antatagonist/AnimalAI/frozen_graph_def.pb to ./models/antatagonist/AnimalAI.nn\n",
      "GLOBALS: 'is_continuous_control', 'version_number', 'memory_size', 'action_output_shape'\n",
      "IN: 'visual_observation_0': [-1, 84, 84, 3] => 'policy/main_graph_0_encoder0/conv_1/BiasAdd'\n",
      "IN: 'vector_observation': [-1, 1, 1, 3] => 'policy/main_graph_0/hidden_0/BiasAdd'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice_1'\n",
      "OUT: 'policy/concat/concat', 'action'\n",
      "DONE: wrote ./models/antatagonist/AnimalAI.nn file.\n",
      "ANTAGONIST: \n",
      "Steps:  510000  Cumulative reward:  -0.9063495246738922  Episode Length:  243.1219512195122\n",
      "Steps:  560000  Cumulative reward:  0.4269206011724635  Episode Length:  -0.8965610498335304\n",
      "Steps:  600000  Cumulative reward:  -0.9986665979959072  Episode Length:  248.0\n",
      "x: 23, y: 2\n",
      "Converting ./models/protagonist/AnimalAI/frozen_graph_def.pb to ./models/protagonist/AnimalAI.nn\n",
      "GLOBALS: 'is_continuous_control', 'version_number', 'memory_size', 'action_output_shape'\n",
      "IN: 'visual_observation_0': [-1, 84, 84, 3] => 'policy/main_graph_0_encoder0/conv_1/BiasAdd'\n",
      "IN: 'vector_observation': [-1, 1, 1, 3] => 'policy/main_graph_0/hidden_0/BiasAdd'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice_1'\n",
      "OUT: 'policy/concat/concat', 'action'\n",
      "DONE: wrote ./models/protagonist/AnimalAI.nn file.\n",
      "PROTAGONIST: \n",
      "Steps:  610000  Cumulative reward:  -0.954073103587711  Episode Length:  243.0731707317073\n",
      "Steps:  660000  Cumulative reward:  -0.8867380292587248  Episode Length:  237.8809523809524\n",
      "Steps:  700000  Cumulative reward:  0.0332451429841218  Episode Length:  144.91176470588235\n",
      "Converting ./models/antatagonist/AnimalAI/frozen_graph_def.pb to ./models/antatagonist/AnimalAI.nn\n",
      "GLOBALS: 'is_continuous_control', 'version_number', 'memory_size', 'action_output_shape'\n",
      "IN: 'visual_observation_0': [-1, 84, 84, 3] => 'policy/main_graph_0_encoder0/conv_1/BiasAdd'\n",
      "IN: 'vector_observation': [-1, 1, 1, 3] => 'policy/main_graph_0/hidden_0/BiasAdd'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice_1'\n",
      "OUT: 'policy/concat/concat', 'action'\n",
      "DONE: wrote ./models/antatagonist/AnimalAI.nn file.\n",
      "ANTAGONIST: \n",
      "Steps:  610000  Cumulative reward:  -0.8050951724761122  Episode Length:  235.42857142857144\n",
      "Steps:  660000  Cumulative reward:  -0.0631886210143214  Episode Length:  188.43396226415092\n",
      "Steps:  700000  Cumulative reward:  0.0610121744430878  Episode Length:  178.34545454545454\n",
      "x: 19, y: 38\n",
      "Converting ./models/protagonist/AnimalAI/frozen_graph_def.pb to ./models/protagonist/AnimalAI.nn\n",
      "GLOBALS: 'is_continuous_control', 'version_number', 'memory_size', 'action_output_shape'\n",
      "IN: 'visual_observation_0': [-1, 84, 84, 3] => 'policy/main_graph_0_encoder0/conv_1/BiasAdd'\n",
      "IN: 'vector_observation': [-1, 1, 1, 3] => 'policy/main_graph_0/hidden_0/BiasAdd'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice_1'\n",
      "OUT: 'policy/concat/concat', 'action'\n",
      "DONE: wrote ./models/protagonist/AnimalAI.nn file.\n",
      "PROTAGONIST: \n",
      "Steps:  710000  Cumulative reward:  -0.9999999310821296  Episode Length:  248.0\n",
      "Steps:  760000  Cumulative reward:  0.2023426656611263  Episode Length:  -0.959125068038702\n",
      "Steps:  800000  Cumulative reward:  -0.4399607280524922  Episode Length:  196.68627450980392\n",
      "Converting ./models/antatagonist/AnimalAI/frozen_graph_def.pb to ./models/antatagonist/AnimalAI.nn\n",
      "GLOBALS: 'is_continuous_control', 'version_number', 'memory_size', 'action_output_shape'\n",
      "IN: 'visual_observation_0': [-1, 84, 84, 3] => 'policy/main_graph_0_encoder0/conv_1/BiasAdd'\n",
      "IN: 'vector_observation': [-1, 1, 1, 3] => 'policy/main_graph_0/hidden_0/BiasAdd'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice_1'\n",
      "OUT: 'policy/concat/concat', 'action'\n",
      "DONE: wrote ./models/antatagonist/AnimalAI.nn file.\n",
      "ANTAGONIST: \n",
      "Steps:  710000  Cumulative reward:  0.0035803035651851  Episode Length:  182.85185185185185\n",
      "Steps:  760000  Cumulative reward:  0.6227417730554301  Episode Length:  88.81981981981981\n",
      "Steps:  800000  Cumulative reward:  0.6573165564919126  Episode Length:  82.66386554621849\n"
     ]
    }
   ],
   "source": [
    "# enter the loop \n",
    "# define configurations\n",
    "trainer_config_path = (\n",
    "    \"configurations/training_configurations/train_ml_agents_config_ppo_10fs.yaml\"\n",
    ")\n",
    "environment_path = \"env/AnimalAI\"\n",
    "run_id_protagonist = \"protagonist\"\n",
    "base_port_protagonist = 5007\n",
    "run_id_antagonist = \"antatagonist\"\n",
    "base_port_antagonist = 6008\n",
    "nb_new_environments = 4\n",
    "\n",
    "from random import randint\n",
    "for i in range(nb_new_environments):\n",
    "    \n",
    "    x = randint(1, 39)\n",
    "    y = randint(1, 39)\n",
    "    if x == 20:\n",
    "        x = 22\n",
    "    if y == 20:\n",
    "        y = 22\n",
    "    print('x: ' + str(x) + ', y: ' + str(y))\n",
    "    \n",
    "    # generate random arena\n",
    "    arena_config = create_arena(x = x, y = y)\n",
    "    \n",
    "    # add steps to the trainer\n",
    "    update_trainer(trainer_config_path)\n",
    "    \n",
    "    # train protagonist\n",
    "    args = RunOptionsAAI(\n",
    "        trainer_config=load_config(trainer_config_path),\n",
    "        env_path=environment_path,\n",
    "        run_id=run_id_protagonist,\n",
    "        base_port=base_port_protagonist + i,\n",
    "        load_model=True,\n",
    "        train_model=True,\n",
    "        arena_config=arena_config \n",
    "    )\n",
    "    run_training_aai(0, args)\n",
    "    \n",
    "    data_path = 'summaries/protagonist_AnimalAI.csv'\n",
    "    df = pd.read_csv(data_path)\n",
    "    print('PROTAGONIST: ')\n",
    "    print('Steps: ', df.loc[0, 'Steps'], ' Cumulative reward: ', df.loc[0, 'Environment/Cumulative Reward'], \\\n",
    "     ' Episode Length: ', df.loc[0, 'Environment/Episode Length'])\n",
    "    print('Steps: ', df.loc[int(len(df)/2), 'Steps'], ' Cumulative reward: ', df.loc[int(len(df)/2), 'Environment/Cumulative Reward'], \\\n",
    "         ' Episode Length: ', df.loc[int(len(df)/2), 'Environment/Episode Length'])\n",
    "    print('Steps: ', df.loc[len(df)-1, 'Steps'], ' Cumulative reward: ', df.loc[len(df)-1, 'Environment/Cumulative Reward'], \\\n",
    "         ' Episode Length: ', df.loc[len(df)-1, 'Environment/Episode Length'])\n",
    "    \n",
    "    # train antagonist\n",
    "    args = RunOptionsAAI(\n",
    "        trainer_config=load_config(trainer_config_path),\n",
    "        env_path=environment_path,\n",
    "        run_id=run_id_antagonist,\n",
    "        base_port=base_port_antagonist + i,\n",
    "        load_model=True,\n",
    "        train_model=True,\n",
    "        arena_config=arena_config \n",
    "    )\n",
    "    run_training_aai(1, args)\n",
    "    \n",
    "    data_path = 'summaries/antatagonist_AnimalAI.csv'\n",
    "    df = pd.read_csv(data_path)\n",
    "    print('ANTAGONIST: ')\n",
    "    print('Steps: ', df.loc[0, 'Steps'], ' Cumulative reward: ', df.loc[0, 'Environment/Cumulative Reward'], \\\n",
    "     ' Episode Length: ', df.loc[0, 'Environment/Episode Length'])\n",
    "    print('Steps: ', df.loc[int(len(df)/2), 'Steps'], ' Cumulative reward: ', df.loc[int(len(df)/2), 'Environment/Cumulative Reward'], \\\n",
    "         ' Episode Length: ', df.loc[int(len(df)/2), 'Environment/Episode Length'])\n",
    "    print('Steps: ', df.loc[len(df)-1, 'Steps'], ' Cumulative reward: ', df.loc[len(df)-1, 'Environment/Cumulative Reward'], \\\n",
    "         ' Episode Length: ', df.loc[len(df)-1, 'Environment/Episode Length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-brave",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
